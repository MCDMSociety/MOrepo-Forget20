---
title: "Computational results (preliminary version)"
description: |
  This report is a preliminary version of the computational results included in the paper
author:
  - name: Nicolas Forget
    url: http://pure.au.dk/portal/en/nforget@econ.au.dk
    affiliation: CORAL, BSS, Aarhus University
    affiliation_url: https://econ.au.dk/coral
  - name: Lars Relund Nielsen
    url: http://pure.au.dk/portal/en/larsrn@econ.au.dk
    affiliation: CORAL, BSS, Aarhus University
    affiliation_url: https://econ.au.dk/coral
  - name: Sune Lauth Gadegaard
    url: http://pure.au.dk/portal/en/sgadegaard@econ.au.dk
    affiliation: CORAL, BSS, Aarhus University
    affiliation_url: https://econ.au.dk/coral
date: "`r Sys.Date()`"
output:
  distill::distill_article:
    toc: true
    toc_depth: 2
bibliography: references.bib
citation_url: https://mcdmsociety.github.io/MOrepo-Forget20/report_paper.html
---

<style type="text/css">
table td, table th {  /* Table  */
  font-size: 10px !important;
}
</style>

```{r setup, include=FALSE}
#' Function for loading missing packages that install them if not already installed.
#'
#' @param packages String vector with package names
#'
#' @return NULL (invisible)
#' @export
#'
#' @examples loadPackages(c("MASS", "ggplot2", "tikzDevice"))
loadPackages <- function(packages) {
  newP <- packages[!(packages %in% installed.packages()[,"Package"])]
  if(length(newP)) install.packages(newP, repos = "http://cran.rstudio.com/")
  lapply(packages, library, character.only = TRUE)
  invisible(NULL)
}
loadPackages(c("tidyverse", "knitr", "rgl", "gMOIP", "rmarkdown", "ggplot2", "plotly", "DT", "RColorBrewer", "wesanderson", "kableExtra", "ggthemes"))

if (isTRUE(getOption('knitr.in.progress'))) options(rgl.useNULL=TRUE)
rgl::setupKnitr()
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning=FALSE, message=FALSE, include = TRUE, 
  cache = TRUE, autodep = TRUE,
  echo=FALSE,
  out.width = "99%", fig.width = 8, fig.align = "center", fig.asp = 0.7,
  layout="l-page"   #"l-screen-inset"
)
knit_hooks$set(webgl = hook_webgl, rgl = hook_rgl)
options(knitr.kable.NA = '')
```

*This report is a preliminary version of the computational results included in the paper. Only instances generated with option spheredown are used since they seems to be hard instances.*

<!-- See https://rstudio.github.io/distill/ on how to write this report. -->

In this section we report on the computational experiments conducted with the tri–objective
branch–and–bound algorithm. 

All instances are converted to minimization problems meaning that if an objective function $z(x)$ should be maximized, $−z(x)$ is minimized instead. 

The purpose of the computational study is to answer the following questions:
  
<!--[Just some ideas for the moment. Add/modify as you like]  -->
  
  1) What is the performance of the different algorithm configurations and which configurations perform the best? In particular, is it worth doing objective branching?
  1) Why does objective branching perform as it does?
  1) Which node selection strategy performs best?
  1) How is the performance of the tri-objective B&B algorithm compared to an objective space search algorithm?
  
  
  
  <!-- 1) How do node selection, (variable selection) and objective branching affect the performance of the algorithm?

  1) Where in the tree do we use obj branching avg + min and max values (relative values and box plot)?
  1) Depth of tree in AP vs KP?
  1) Do integer rounding improve the performance? -->
  
  

<!--   3. Which algorithm configuration is best? -->
<!--      a) Is there a clear winner? -->
<!--      b) Is the best node selection strategy affected by other algorithm configurations? -->
<!--         * Are some problem classes solved best with one node selection strategy compared to others? -->
<!--         * Are 'easy' problems solved best with one node selection strategy compared to others? -->
<!--      c) Does different `OB` strategies affect the node selection strategy? -->

<!--   4. How are nodes pruned? -->

<!-- The primary focus is on computational time and number of nodes explored the size of the branching -->
<!-- tree before the algorithm ends. -->

<!-- The second purpose of this study is to learn how the characteristics of an instance can affect its -->
<!-- difficulty. The difficulty will be here measured by the *size of the non-dominated set*, and the -->
<!-- *computational time* required to solve the instance. The computational time will be determined -->
<!-- using the branch and bound algorithm previously described. Note that the complexity of an objective -->
<!-- space search algorithm is positively correlated to the size of the non-dominated set as the more -->
<!-- non-dominated there are, the more integer programs have to be solved. -->

# Impelmentation details and algorithm configurations

All algorithms have been implemented in Julia $1.0.1$. The experiments have been done a computer with an Intel(R) Core(TM) i7-4785T CPU @ 2.20GHz processor and 16GB of RAM memory, using Linux Ubuntu 14.04 LTS.

In order to compute the linear relaxation at each node, the solver [Bensolve](https://www.optimierung-loehne.uni-jena.de/bensolve) is used [@Bensolve]. To avoid reading and writing text files at each node of the tree, we have implemented a wrapper that calls Bensolve, retrieve some outputs and insert them directly in our code as matrices. 
Numerical instabilities leading to missing non-dominated points in the final output have been detected while building the hyperplanes representation of the lower bound set. The matrix used when finding the normal to each hyperplane may in a few cases be close to singular (its determinant is close to zero). Hence a small value `eps` has been introduced so that if the determinant is less than or equal `eps`, then the hyperplane is discarded, thus leaving a weaker but valid lower bound set. For more information, we refer the reader to the [section about numerical instabilities](https://mcdmsociety.github.io/MOrepo-Forget20/report.html#numerical-instablities) in @Forget20a. We have used `eps` $= 0.001$ in all tests reported in this paper resulting in that no missing non-dominated points have been detected.

The variable selected in Step 5 of the algorithm differs depending on whether objective branching is applied or not. If no objective branching is performed, the algorithm will branch on the free variable that is the most often fractional among the extreme points of the lower bound set, given that at least one of the variables has a fractional value. If no variable has a fractional value in any of the extreme points, the variable that is the most often different (i.e. with the average value closest to $0.5$) is chosen. If objective branching is enabled, the rule is the same, except that a different variable may be chosen in each sub-problem. Indeed, given a sub-problem $P(\eta,s)$ in the objective space, only the extreme points of the lower bound set included in $P(\eta,s)$ (i.e. that dominates $s$) will be considered. In case there are multiple possible choices or no extreme point included in $P(\eta,s)$, the variable with the smallest index will be chosen.

To test different algorithm configurations we use the following parameters:

* Two node selection methods are considered (Step 1 of Algorithm 1):
  + `B`: breadth first strategy.
  + `D`: depth first strategy.
* Three objective branching strategies are considered:
  + `N`: no objective branching is performed. This is the equivalent of skipping Step 4 of Algorithm 1. 
  + `F`: full objective branching using super local upper bounds (see Algorithm 2). 
  + `C`: objective branching using a single cone, namely, the nadir point of the local upper bounds dominated by the lower bound set (see Section 4.3).

This leads to 6 configurations: `B|N`, `B|F`, `B|C`, `D|N`, `D|F` and `D|C`. For an overview of more configurations such as different variable selection rules (Step 5 of the algorithm), the reader is referred to @Forget20a.

The Objective Space Search (OSS) algorithm of @Tamby was implemented. A short description of the algorithm is given in the appendix. As the LP solver in Bensolve is GLPK, the same MIP solver was used. Due to technical constraints in the chosen programming language (Julia), the MIP solver could not be warm started. Warm starting the MIP solver would lead to improved cpu times. However, note that the same holds for the branch and bound algorithm which also does not use warm starting when calculating the lower bound set in each node of the branching tree.  

All algorithms and configurations have been tested using a time limit of half an hour (1800 seconds).


# Test instances

```{r load instance results}
toLink <- function(inst) {
  if (length(inst) == 0) return("")
  links <- str_c('../../docs/instances/', inst, '.html')
  url <- str_c('instances/', inst, '.html')
  if_else(file.exists(links), str_c('<a href="', url, '">', inst, '</a>'), inst)
}

limSec <- c(0, 30*60)  # computation time limits (exclude instances with max cpu < limSec[1] or  min cpu > limSec[2])
abbrv <- function(str) {
  str_replace_all(str, 
    c("breadth" = "B", "depth" = "D", "mof" = "F", "mfavg" = "A", "exact" = "F", 
      "cone" = "C", "none" = "N", "None" = "N", "spheredown" = "down", "sphereup" = "up"))
}
datAll <- read_csv("../statistics.csv") %>%   #"../convert/data/stat.csv"
  filter(coef == "spheredown", rangeC == "[1,1000]" | rangeC == "[1,1000]|[1,100]", varsel == "mof") %>% #| coef == "sphereup"
  mutate(YNsRatio = YNs/YN, 
         YNusRatio = 1-YNs/YN, 
         YNsneRatio = (YNs-YNse)/YN,
         algConfig = tolower(str_c(nodesel, varsel, OB, sep="|")),
         nodeselVarsel = tolower(str_c(nodesel, sep="|")),   #nodeselVarsel = tolower(str_c(nodesel, varsel, sep="|")),
         resultName = str_c(instance, algConfig, sep="_")) %>% 
  mutate(algConfig = tolower(str_c(nodesel, OB, sep="|")))
  # group_by(instance) %>% 
  # mutate(minCpu = min(tpstotal), maxCpu = max(tpstotal)) %>% 
  # mutate(unsolvedAllConfigs = if_else(maxCpu < limSec[1] | minCpu >= limSec[2], T, F))
algConfigsN <- length(unique(datAll$algConfig))

# tmp <- datAll %>% 
#   group_by(instance) %>% 
#   summarise(minCpu = min(tpstotal), maxCpu = max(tpstotal)) %>% 
#   filter(maxCpu < limSec[1] | minCpu > limSec[2]) %>% 
#   pull(instance)

tmp <- datAll %>% 
  group_by(pb, n, coef) %>% 
  mutate(allUnsolved = 1 - max(solved)) %>% 
  ungroup() %>% 
  filter(allUnsolved == 1) %>% 
  pull(instance)

datAll <- datAll %>% 
  filter(!(instance %in% tmp)) %>%
  mutate(solved = if_else(tpstotal >= limSec[2] | solved == 0, 0, 1)) %>% 
  # group_by(instance) %>% 
  # filter(n() == algConfigsN) %>% 
  # ungroup() %>% 
  mutate(tpstotal = if_else(tpstotal >= limSec[2], limSec[2], tpstotal),
         algConfig = abbrv(algConfig),
         coef = abbrv(coef),
         nodeselVarsel = abbrv(nodeselVarsel),
         OB = abbrv(OB))
# View(datAll %>% group_by(instance) %>% summarise(ctr = n()))
algConfigs <- unique(datAll$algConfig)
nodeselVarselConfigs <- unique(datAll$nodeselVarsel)
datNotSolved <- datAll %>% filter(solved == 0) 
datSolved <- datAll %>% filter(solved == 1)
datInput <- datAll %>% 
  group_by(instance) %>%
  mutate(unsolved = 1-max(solved)) %>%
  ungroup() %>%
  distinct(instance, .keep_all = TRUE) %>% 
  select(instance, pb, n, p, coef, contains("range"), ratioNDcoef, unsolved)
# instances with all configs run

datWin <- 
  datAll %>% 
  group_by(instance) %>% 
  nest() %>% 
  mutate(winner = map(data, function(df) {df %>% arrange(tpstotal) %>% slice(1)})) %>% 
  mutate(win = map(winner, function(df) {if_else(df$solved == 1, df$algConfig, "Unsolved")})) %>% 
  select(-winner) %>% 
  unnest(c(data, win)) 

datNotStable <- read_csv("../convert/data/stat.csv", col_types = cols()) %>%
  filter(coef == "spheredown", rangemax == 1000, solved == 1) %>% #| coef == "sphereup"
  group_by(instance) %>%
  distinct(YN) %>%
  summarise(ctr = n()) %>%
  dplyr::filter(ctr > 1)
if (nrow(datNotStable) > 0) stop("Instabilities found!")

datObNC <- datAll %>% filter(OB == "N" | OB == "C")

tmp <- tibble(algConfig = datAll %>% pull(algConfig) %>% unique()) %>% separate(algConfig, c("nodeselVarsel", "OB"), remove = F, sep = "\\|")
datAllJoined <- datAll %>% 
  group_by(instance) %>% 
  nest() %>% 
  mutate(missing = map(data, function(df) full_join(df, tmp, by = c("algConfig", "nodeselVarsel", "OB")) %>% mutate(pb = df$pb[1]))) %>% 
  select(-data) %>% 
  unnest(missing) %>% 
  replace_na(list(tpstotal = 1800)) 

datOSS <- read_csv("../statistics_oss.csv") 
```

```{r Define theme and color scales}
theme_publish <- function() {
  library(grid)
  library(ggthemes)
  return(theme_foundation(base_size = 10) +
    theme(
      # plot.title = element_text(face = "bold", size = rel(1.2), hjust = 0.5),
      text = element_text(face = "plain"),
      # panel.background = element_rect(colour = NA),
      panel.spacing = unit(0.5, "cm"),
      # plot.background = element_rect(colour = NA),
      # panel.border = element_rect(colour = NA),
      axis.title = element_text(face = "plain",size = rel(1)),
      # axis.title.y = element_text(angle=90,vjust =2),
      # axis.title.x = element_text(vjust = -0.2),
      axis.text = element_text(),
      axis.line = element_line(colour="black"),
      axis.ticks = element_line(),
      panel.grid.major = element_line(colour="#f0f0f0"),
      panel.grid.minor = element_blank(),
      legend.key = element_rect(colour = NA),
      legend.position = "bottom",
      legend.direction = "horizontal",
      # legend.key.size = unit(0.2, "cm"),
      legend.key.height = unit(0.2, "cm"), 
      legend.key.width = unit(0.75, "cm"),
      # legend.margin = unit(0, "cm"),
      legend.title = element_blank(), #element_text(face="italic"),
      # plot.margin=unit(c(10,5,5,5),"mm"),
      # strip.background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
      # strip.text = element_text(face="bold"),
    )
  )
}


# scale_fill_Publication <- function(...){
#       library(scales)
#       discrete_scale("fill","Publication",manual_pal(values = c("#386cb0","#fdb462","#7fc97f","#ef3b2c","#662506","#a6cee3","#fb9a99","#984ea3","#ffff33")), ...)
# 
# }
# 
# scale_colour_Publication <- function(...){
#       library(scales)
#       discrete_scale("colour","Publication",manual_pal(values = c("#386cb0","#fdb462","#7fc97f","#ef3b2c","#662506","#a6cee3","#fb9a99","#984ea3","#ffff33")), ...)
# 
# }



# display.brewer.pal(n = 11, name = "RdYlBu")
pal <- brewer.pal(n = 11, name = "RdYlBu")
# algConfigs <- algConfigs[c(2,5, 3,6, 1,4, 8,11, 9,12, 7,10)]
palAlgConfigs <- wes_palette("Zissou1", algConfigsN, type = "continuous")
palAlgConfigs <- c(brewer.pal(n =  algConfigsN/2, name = "Reds"), brewer.pal(n =  algConfigsN/2, name = "Blues"))
# display.brewer.pal(n = 9, name = "Blues")
# brewer.pal(n = 6, name = "Greens")

scale_color_algConfig <- scale_color_manual(
  values = setNames(palAlgConfigs, algConfigs),
  drop = F)
scale_fill_algConfig <- scale_fill_manual(
  values = setNames(palAlgConfigs, algConfigs),
  drop = F)

scale_color_nodesel_varsel <- scale_color_manual(
  values = setNames(pal[c(1,3,9,11)], nodeselVarselConfigs),
  drop = F)

scale_color_ob <- scale_color_manual(
  values = setNames(pal[c(1,3,9,11)], c("C", "F", "N", "OSS")),
  drop = F)

scale_fill_ob <- scale_fill_manual(
  values = setNames(pal[c(1,3,9,11)], c("C", "F", "N", "Unsolved")),
  drop = F)

scale_linetype_ob <- scale_linetype_manual(
  values = c("C" = 1, "F" = 2, "N" = 3),
  drop = F)

# scale_color_nodesel <- scale_color_manual(
#   values = c("breadth" = "red", "depth" = "green"),
#   drop = F)

# scale_alpha_varsel <- scale_alpha_manual(
#   values = c("mof" = 1, "mfavg" = 0.75),
#   drop = F)

# scale_linetype_valsel <- scale_linetype_manual(
#   values = c("mof" = 1, "mfavg" = 2),
#   drop = F)
  

scale_linetype_nodesel_varsel <- scale_linetype_manual(
  values = c("B" = 1, "D" = 2, "OSS" = 3, "Unsolved" = 1),
  drop = F)
```

```{r tabInput, layout="l-screen-inset",}
tabInput <- datInput %>% 
  group_by(pb, n, coef) %>% 
  summarise(instances = n(), 
            rangeCoef = rangeC[1], 
            gapO = mean(c(rangeGapZ1, rangeGapZ2, rangeGapZ3)), 
            ratioNDCoef = mean(ratioNDcoef), 
            someUnsolved = max(unsolved)) %>% 
  ungroup() %>% 
  group_by(pb) %>% 
  summarise(n = str_c(n, collapse = ", "), 
            instances = mean(instances), 
            range = rangeCoef[1], ratio = mean(ratioNDCoef)
            ) 
  # mutate(n = str_c(n, if_else(someUnsolved == 1, "*", "")))
# tabInput %>% 
#   select(-pb, -coef, -someUnsolved, -gapO, -rangeCoef, -ratioNDCoef) %>%
#   kable(digits = c(0,0), col.names = c("$n$", "#"),
#         caption = "Instances used ($n$: number of variables, #: number of instances.") %>% 
#   kable_styling(full_width = T) %>% 
#   pack_rows("AP", min(which(tabInput$pb == "AP")), max(which(tabInput$pb == "AP"))) %>% 
#   pack_rows("KP", min(which(tabInput$pb == "KP")), max(which(tabInput$pb == "KP"))) %>% 
#   pack_rows("UFLP", min(which(tabInput$pb == "UFLP")), max(which(tabInput$pb == "UFLP"))) 

tabInput %>% 
  select(-range, -ratio) %>% 
  group_by(pb) %>% 
  kable(digits = c(0,0,1), 
        escape = F,
        col.names = c("", str_c("$n$", footnote_marker_alphabet(1)), str_c("#", footnote_marker_alphabet(2))),
        caption = "Instances used.") %>% 
  add_footnote(label = c("Number of variables.", "Number of instances for each variable size")) %>% 
  kable_styling(full_width = T) 
```

A total of `r nrow(datInput)` instances (see Table \@ref(tab:tabInput)) has been generated. Three problem classes are considered: the linear assignment problem (AP), the knapsack problem (KP) and the uncapacitated facility location problem (UFLP). The mathematical programming formulation for each problem is given in the appendix. The number of variables in each problem class has been increased until no algorithm configurations was able to compute an optimal solution within a time limit of half an hour (1800 seconds).

The objective coefficients are generated in the range [1, 1000] on the lower part of a 3D sphere using the R package @gMOIP. Hence a high number of the coefficient vectors for the variables are non-dominated among each other (`r round(100 * tabInput %>% filter(pb == "AP") %>% pull(ratio))`% for AP and `r round(100 * tabInput %>% filter(pb == "KP") %>% pull(ratio))`% for KP). This way of generating the objective coefficients has been tested against other methods and seems to result in optimal solutions with a high number of non-dominated points [@Forget20a] (implying hard instances). For UFLP instances the same generation method has been used. However, since two cost groups exists (see the appendix), a range of [1, 1000] has been used for generating the cost of assigning a customer to a service point and a range of [1, 100] for generating the cost for opening a service point. The objective coefficients are all integer.

For the AP the constraints are fixed given the problem size. The same holds for the UFLP when assuming the number of facilities equals the number of customers. For KP instances, the integer coefficients of the constraint are generated randomly in the range [1,15]. The right-hand side is set equal to half of the sum of the coefficients on the right hand side, rounded down if not integer. For KP three different constraints are generated for each variable size and objective coefficients.

All instances are public available at Multi-Objective Optimization Repository (MOrepo) [@MOrepo] in a sub-repository [@MOrepo-Forget20]. Moreover the repository also contains further instances used for preliminary testing. An overview of all the results are given in [@Forget20a]. The non-dominated sets for each instance is available together with detailed results and plots of some of the instances @MOrepo-Forget20-Details.     



# Performance of the different algorithm configurations

```{r}
tmp <- datAllJoined %>% 
  group_by(algConfig) %>% 
  summarize(cpu = mean(tpstotal)) %>% 
  mutate(pct = cpu/min(cpu), strV = str_c(algConfig, " (", round((pct-1) * 100), "%)")) %>% 
  arrange(cpu) 
winSq <- tmp %>% pull(algConfig)
tmp <- tmp %>% pull(strV)
# datAll %>% group_by(algConfig) %>% summarize(cpu = mean(tpstotal)) %>% arrange(cpu) %>% kable()
# datAll %>% group_by(algConfig, pb) %>% summarize(cpu = mean(tpstotal)) %>% arrange(pb, cpu) %>% kable()
```

First, we rank the configurations with respect to mean cpu time for all instances. The sequence from best to worst becomes `r tmp` where the increase in percent compared to the best configuration is given in parentheses. Note that the mean cpu times calculated is in fact a lower bound due to the total run time limit. 

Second, a comparison of the different algorithm configurations given problem class can be seen in Figure \@ref(fig:perfPlotPct). Remark that we have increased the variable size for each problem class until the size becomes so big that some instances cannot be solved within the time limit. That is, the number of instances solved before the time limit are under 100%.   

```{r}
tmp <- datAll %>%
  group_by(algConfig, pb) %>%
  arrange(tpstotal) %>%
  mutate(count = row_number(), total = n()) %>% 
  group_by(pb) %>% 
  mutate(total = max(total)) %>% 
  group_by(pb, algConfig, tpstotal) %>% 
  arrange(pb, algConfig, tpstotal, count) %>% 
  filter(tpstotal < 1800 | row_number() == 1) %>% 
  mutate(pct = count/total) %>% 
  select(pb, algConfig, tpstotal, count, pct, total, OB, nodeselVarsel)
```


```{r perfPlot, eval=FALSE, fig.asp=1.2, fig.cap="Performance profile: Number of instances in percent solved given cpu time. An instance is considered as unsolved if the cpu time exceeds 1800 seconds (time limit)."}
ggplot(tmp) +
  geom_step(aes(x=tpstotal, y=count, color = OB, linetype = nodeselVarsel)) +
  facet_grid(rows = vars(pb), scales = "free") +
  # ggtitle(str_c("Number of instances solved within a given cpu time")) +
  labs(color = "nS:", linetype = "oB:") +
  scale_color_ob + scale_linetype_nodesel_varsel +
  theme_publish() + xlab("cpu") + ylab("count")
```

```{r perfPlotPct, fig.asp=1, fig.cap="Performance profile: Number of instances in percent solved given cpu time. An instance is considered as unsolved if the cpu time exceed 1800 seconds (time limit)."}
ggplot(tmp) +
  # geom_step(aes(x=tpstotal, y=count, color = nodeselVarsel, linetype = OB)) +
  geom_step(aes(y=pct, x=tpstotal, color = OB, linetype = nodeselVarsel), alpha = 0.75) +
  # geom_point(aes(y=..y.., x=tpstotal, color = OB, linetype = nodeselVarsel), stat="ecdf", size = 1) +
  facet_grid(rows = vars(pb)) +
  # ggtitle(str_c("Number of instances solved within a given cpu time")) +
  labs(color = "nS:", linetype = "oB:") +
  scale_color_ob + scale_linetype_nodesel_varsel +
  xlab("cpu (seconds)") + ylab("%") +
  theme_publish() +
  coord_cartesian(expand = FALSE, ylim = c(0, NA), xlim = c(-10, 1797))
```

Some observations based on Figure \@ref(fig:perfPlotPct) are:

* In general full objective branching (`B|F` and `D|F`) performs poorly compared to the other configurations. This is especially evident for AP and UFLP. For KP full objective branching are still among the worst ones. 

* For both KP and UFLP, the best objective branching configuration is using a single cone (`C`), while no objective branching (`N`) performs best for AP. 

* In general node selection using breadth first strategies tend to perform better than depth first strategies given the objective branching strategy. 

These observations will be explored with more details in the next sections. 

Finally, consider the winning algorithm configurations in Figure \@ref(fig:winConfigs). Note that for KP and UFLP using no objective branching is competitive for some of the smaller instances. Furthermore, in general there is not configuration that wins. That is, algorithms using different node selection and objective branching strategies during the solution procedure may be useful. 

```{r winConfigs, fig.cap="Proportion of winning algorithm configurations."}
res <- datWin %>% 
  group_by(instance) %>%
  # filter(n() == algConfigsN, min(solved) == 1) %>% 
  group_by(pb, n) %>% 
  mutate(count = n()) %>% 
  group_by(pb, n, win) %>% summarize(pct = n()/mean(count)) %>% 
  arrange(pb, n, desc(pct)) %>% 
  separate(win, into = c("nodeselVarsel","OB"), sep = "\\|", remove = F) %>% 
  mutate(OB = replace_na(OB, "Unsolved"))
ggplot(res, aes(x = factor(n), y = pct, fill = OB, linetype = nodeselVarsel)) + 
  geom_col(color = "black") + 
  geom_text(aes(label = round(pct, 2)), colour="white", size=2.5,
              position = position_stack(vjust = .5)) +
  scale_fill_ob + scale_linetype_nodesel_varsel +
  facet_grid(cols = vars(pb), margins = F, scales = "free") +
  ylab("% of instances") + xlab("n") +
  theme_publish()
```


# Objective branching: a closer look

For taking a closer look at the different objective branching configurations we limit us to the set of instances which have been solved to optimality for all algorithm configurations. Detailed summary statistics are given in Table \@ref(tab:resTable). 

```{r resTable, layout="l-screen-inset"}
dat <- datAll %>% 
  group_by(instance) %>%
  filter(n() == algConfigsN, min(solved) == 1) %>%
  ungroup() 
winSeq <- unique(dat$algConfig)
getResGroup <- function(dat, ...) {
  # winSeq <- dat %>% 
  #   group_by(algConfig) %>% 
  #   summarize(cpu = mean(tpstotal)) %>% 
  #   mutate(pct = cpu/min(cpu), strV = str_c(algConfig, " (", round((pct-1) * 100), "%)")) %>% 
  #   arrange(cpu) %>% pull(algConfig)
  cols = c("cpu", "cpuLB", "nodes", "prune")  # columns for each group
  colN <- purrr::flatten_chr(map(cols, function(x) str_c(x, winSeq, sep = "_")))
  idx <- {
    lgd <- length(winSeq)
    idx <- NULL
    for (i in 1:length(winSeq))
      for (j in 1:length(cols))
        idx <- c(idx, i + (j-1)*lgd)
    idx
  }
  colN <- colN[idx]
  
  datYN <- dat %>% 
    group_by(instance, ...) %>% 
    summarise(YN = max(YN), YNse = max(YNse), YNs = max(YNs)) %>% 
    group_by(...) %>% 
    summarise(YN = mean(YN), YNse = mean(YNse), YNs = mean(YNs)) 

  datResults <- dat %>% 
    group_by(..., algConfig) %>% 
    summarise(ct = n(), cpu = mean(tpstotal), cpuMax = max(tpstotal), cpuMin = min(tpstotal), 
              nodes = mean(nbnodes), nInf = mean(pctinfeas), nOpt = mean(pctopt), nDom = mean(pctdomi),
              dptLeaf = mean(avgdepthT), dptMinLeaf = mean(mindepthT), dptMaxLeaf = mean(maxdepthT),
              solved = min(solved), cpuLB = 1000*mean(tpsLB/nbLB)) %>% 
    full_join(datYN) %>% 
    mutate(cpu = if_else(cpu == min(cpu), str_c(round(cpu, 1), "!"), str_c(round(cpu, 1))),
           cpuLB = if_else(cpuLB == min(cpuLB), str_c(round(cpuLB, 1), "!"), str_c(round(cpuLB, 1))),
           nodes = if_else(nodes == min(nodes), str_c(round(nodes, 0), "!"), str_c(round(nodes, 0)))) %>% 
    ungroup() %>% 
    mutate(cpu = if_else(solved == 1, 
                         str_c(cpu, " [", round(cpuMin,1), ",", round(cpuMax,1), "]"), 
                         str_c(cpu, "*", " [", round(cpuMin,1), ",", round(cpuMax,1), "]")),
           YN = str_c(round(YN), " (", round(100*YNse/YN), "/", round(100*(YNs-YNse)/YN), "/", round(100*(YN-YNs)/YN), ")"),
           nodes = str_c(nodes, " [", round(dptMinLeaf), ",", round(dptLeaf), ",", round(dptMaxLeaf), "]"),
           prune = str_c("[", round(nInf), ",", round(nOpt), ",", round(nDom), "]")
           ) %>%
    select(-solved, -cpuMin, -cpuMax, -dptMinLeaf, -dptMaxLeaf, -dptLeaf, -nInf, -nOpt, -nDom) %>%
    pivot_wider(names_from = c(algConfig), values_from = c(cpu, cpuLB, nodes, prune)) %>%
    select(..., ct, YN, !!colN)
  return(datResults)
}
datResRows <- getResGroup(dat, pb, n)
tabResults <- NULL
for (p in unique(datResRows$pb)) {
  dat1 <- datResRows %>% filter(pb == p)
  dat2 <- getResGroup(dat, pb) %>% filter(pb == p)
  tabResults  <- bind_rows(tabResults, dat1, dat2)
}

colsN = c("cpu", "cpuLB", "nodes", "prune")
tabResults <- tabResults %>% 
  mutate_at(vars(contains(c("cpu", "cpuLB", "nodes"))), ~cell_spec(., bold = if_else(str_detect(., fixed("!")), T, F))) %>% 
  mutate_if(is.character, str_replace_all, pattern = "!", replacement = "")
# digits <- c(0,0,0,rep(c(1,0,0),algConfigsN))
tabResults %>% 
  select(-pb) %>% 
  kable(
    # digits = digits, 
    escape = F,
    col.names = c(str_c("n", footnote_marker_alphabet(1)), 
                  str_c("#", footnote_marker_alphabet(2)), 
                  str_c("|YN|", footnote_marker_alphabet(3)),  
                  str_c(rep(colsN, length(winSeq)),  
                        c(footnote_marker_alphabet(4),
                          footnote_marker_alphabet(5),
                          footnote_marker_alphabet(6),
                          footnote_marker_alphabet(7)))
    ),
    caption = "Detailed results for all instances which have been solved to optimality for all algorithm configurtions. The winner among the configurations for a column has been highlighted in bold. Summary statistics for each problem class is given in a grey row.") %>% 
  add_footnote(label = c("Number of variables.", 
                         "Number of instances.", 
                         "Avg. number of non-dominated points (supported extreme, supported non-extreme and unsupported in percent).",
                         "Avg. cpu time (seconds). Square brackets contains the range.",
                         "Avg. cpu time (miliseconds) used to find the LB set per node where LB set found.",
                         "Avg. number of nodes in the branching tree. Square brackets contains the min, avg. and max depth of leaf nodes.",
                         "Percentages of leaf nodes pruned by infisibility, optimality and dominance."
                         )
               ) %>% 
  kable_styling() %>% 
  add_header_above(c(" " = 3, setNames(rep(length(colsN), length(winSeq)), winSeq))) %>% 
  pack_rows("AP", min(which(tabResults$pb == "AP")), max(which(tabResults$pb == "AP"))) %>% 
  pack_rows("KP", min(which(tabResults$pb == "KP")), max(which(tabResults$pb == "KP"))) %>% 
  pack_rows("UFLP", min(which(tabResults$pb == "UFLP")), max(which(tabResults$pb == "UFLP"))) %>% 
  row_spec(c(max(which(tabResults$pb == "AP")), 
             max(which(tabResults$pb == "KP")), 
             max(which(tabResults$pb == "UFLP"))), italic = T, background = "lightgrey") %>% 
  scroll_box(width = "100%")
```

```{r}
tmp <- datAll %>% 
  group_by(instance) %>%
  filter(n() == algConfigsN) %>%
  ungroup()  %>% 
  filter(OB == "F") %>% 
  # group_by(pb) %>% 
  summarise(avgnbpbOB = mean(avgnbpbOB)) %>% pull(avgnbpbOB)
```

Figure \@ref(fig:perfPlotPct) illustrates that using full objective branching (`F`) is not efficient with respect to cpu time. As explained in Section 4.3, full objective branching creates more nodes (in average `r round(tmp,1)`) in the branch and bound tree as it partitions the objective space in smaller regions. This seems to lead to longer computation times, since computing the lower bound set of each node is computationally very expensive. Indeed, this can be seen in Figure \@ref(fig:figTreeSize) where `F` configurations produce more nodes in the tree compared to `C` and `N` given same node selection rule. This observation makes sense, as the purpose of objective branching as described in Algorithm 2 is to produce more sub-problems and thus get wider trees, hoping that these sub-problems will be easier and that the tree will have a smaller depth. 
<!-- In practice, as it can be seen in Table \@ref(tab:resTable), in general, `F` configurations indeed produce more nodes in the tree than the other configurations.  -->
Furthermore, the branching nodes produced using full objective branching takes longer to process, on average. This can be seen in Table \@ref(tab:resTable) if we compare the time used to find lower bound sets for configurations `F` and `C`. That is, the sub-problems generated in the `F` configurations tends to be harder to solve due to a more complicated set of constraints. 

```{r figTreeSize, fig.cap="Average branching tree size."}
datAll %>% 
  group_by(instance) %>%
  filter(n() == algConfigsN, min(solved) == 1) %>%
  ungroup() %>% 
  group_by(pb, OB, nodeselVarsel) %>% 
  summarise(node = mean(nbnodes)) %>% 
  group_by(pb) %>% 
  arrange(node) %>% 
  ggplot(aes(x = pb, y = node, fill = OB, linetype = nodeselVarsel)) + 
  geom_col(position = "dodge2", color = "black") +
  # geom_line(alpha = 0.75) +
  # geom_point(alpha = 0.75) +
  facet_grid(cols = vars(pb), scales = "free") + 
  # ggtitle(str_c("Average branching tree size.")) +
  labs(color = "oB:", linetype = "nS:") +
  scale_fill_ob + scale_linetype_nodesel_varsel +
  theme_publish() + theme() +
  xlab("problem class") + ylab("tree size") 
```

```{r}
tmp <- round(100*mean(dat$tpsLB/dat$tpstotal))
```

To make full branching competitive the cpu time for calculating the lower bound set need to be reduced. That is, computing the lower bound set faster than what is currently done using BenSolve. This would lead to large reductions in the overall computation time as most of the computational time is spent in computing the lower bound sets (approx. `r tmp`%). Some ideas for reducing the cpu time could be to:

*	implement a way to warm start the LP solver for each sub-problem,
* introduce an updating procedure of the lower bound set, instead of a computation from scratch at each node,
* use further pruning rules that discard sub-problems before calculating the lower bound set.

Updating and pruning procedures may include using the local information available in the subproblems with respect to the region in the objective space defined by a branching node. In @Gupte19 it was shown that presolve techniques, contrary to the case of single-objective optimization, has a great effect deeper in the search tree. Hence, one could imagine that the solutions are more likely to be close in a specific part of the objective space and thus, by localizing the branch and bound, presolving the nodes or introducing cuts may have a significant impact.

Except for AP, objective branching using a single cone (`C`) are performing better than the `N` configurations in both cpu time and number of tree nodes. That is, objective branching is competitive. A first explanation of the efficiency of `C` for KP and UFLP is that for a given node $\eta$, the `C` configurations will focus on a specific part of the objective space (a cone defined by $d^N(\eta)$) for the same number of sub-problems created (2, in the decision space). Thus, the lower bound sets tends to be less complex (some areas of the objective space are discarded before computation, while it is fully considered in the `N` version), which means possibly spending less time in the most expensive part of the branch and bound (computing the lower bound sets). Second, the way nodes are fathomed is highly influenced by the choice of objective branching strategy. Indeed, as it can be seen in Table \@ref(tab:resTable), in the `C` configurations, the proportion of nodes fathomed by infeasibility tends to be much higher than for the `N` configurations. This is due to the fact that additional constraints added in the `C` configurations will make sub-problems more likely to be infeasible. Moreover, fathoming a node by infeasibility is faster than fathoming a node by optimality or by dominance. To fathom by dominance or optimality, having the lower bound set computed is required. On the contrary, testing the feasibility of the sub-problem is the first thing done when a node is being explored.

The case of the AP is different from KP and UFLP. This is not surprising as the constraint matrix of the AP is totally unimodular implying the extreme points of the the polytope defined by the constraints are integral. Hence, all the extreme points of the lower bound sets correspond to feasible solutions in the `N` configurations. This implies that in Step 5, the branching rule will exploit this and thus, always branch on the most often different variable. On the contrary, as new constraints are added in the `C` configurations, the extreme points are not necessarily integer and the branching rule will choose to branch on the most often fractional variable. This difference in the branching rule may have a significant impact and thus, explain why for AP `N` configurations are better.









# Node selection: breadth vs depth

In Figure \@ref(fig:perfPlotPct), it can be seen that node selection using a breadth first strategy tends to perform better than a depth first strategy given an objective branching rule. This observation is in contrast to the prevalent use of depth first in multi-objective branch and bound algorithms [@Przybylski17]. 

```{r perfPlotND, fig.cap="Smoothed curves of relative number of non-dominated points compared to relative cpu. "}
# instancesAllSolved <- datAll %>% 
#   group_by(instance) %>%
#   filter(n() == algConfigsN, min(solved) == 1) %>% 
#   pull(instance)
# instancesNCSolved <- datAll %>%
#   filter(OB == "C" | OB == "N") %>% 
#   group_by(instance) %>%
#   filter(min(solved) == 1) %>% 
#   pull(instance)
# 
# csvFiles <- list.files("..", ".csv", full.names = T) %>% 
#   str_subset("nd")
# dat <- NULL
# for (f in csvFiles) dat <- bind_rows(dat, read_csv(f))
# dat <- dat %>% mutate(algConfig = abbrv(algConfig))
# 
# # dat1 <- dat %>% 
# #   separate(algConfig, into = c("nodesel", "varsel", "OB"), sep = "_", remove = F) %>%
# #   mutate(pb = str_replace(instance, "^Forget20-(.*?)_.*$", "\\1")) %>% 
# #   filter(instance %in% instancesAllSolved, varsel == "mof")
# # 
# # ggplot(aes(x = pctCpu, y = pct, color = nodesel), data = dat1) + 
# #   geom_smooth() + 
# #   facet_grid(rows = vars(pb)) 
# 
# # ggplot(aes(x = pctCpu, y = pct, color = algConfig), data = dat1) + 
# #   geom_smooth() + 
# #   facet_grid(rows = vars(pb)) 
# 
# dat1 <- dat %>% 
#   separate(algConfig, into = c("nodesel", "varsel", "OB"), sep = "_", remove = F) %>%
#   mutate(pb = str_replace(instance, "^Forget20-(.*?)_.*$", "\\1")) %>% 
#   filter(instance %in% instancesNCSolved, varsel == "F", OB == "N" | OB =="C")
# 
# ggplot(aes(x = pctCpu, y = pct, linetype = nodesel), data = dat1) + 
#   geom_smooth(color = "black", se = F, size = 0.5) + 
#   # facet_grid(rows = vars(pb)) +
#   coord_cartesian(expand = FALSE, ylim = c(0, 1), xlim = c(0, 1)) +
#   # ggtitle(str_c("Number of instances solved within a given cpu time")) +
#   labs(linetype = "") +
#   scale_linetype_nodesel_varsel + 
#   theme_publish() + ylab("% of non-dominated points") + xlab("% of cpu time")


instancesNCSolved <- datAll %>%
  filter(OB == "C" | OB == "N") %>% 
  group_by(instance) %>%
  filter(min(solved) == 1) %>% 
  pull(instance)

csvFiles <- list.files("..", ".csv", full.names = T) %>% 
  str_subset("nd")
dat <- NULL
for (f in csvFiles) dat <- bind_rows(dat, read_csv(f))
dat <- dat %>% mutate(algConfig = abbrv(algConfig))
dat1 <- dat %>% 
  separate(algConfig, into = c("nodesel", "varsel", "OB"), sep = "_", remove = F) %>%
  mutate(pb = str_replace(instance, "^Forget20-(.*?)_.*$", "\\1")) %>% 
  filter(instance %in% instancesNCSolved, varsel == "F", OB == "N" | OB =="C") 
  
csvFiles <- list.files("..", ".csv", full.names = T) %>% 
  str_subset("points_oss")
dat <- NULL
for (f in csvFiles) dat <- bind_rows(dat, read_csv(f))
dat1 <- bind_rows(dat1, dat) %>% 
  mutate(nodesel = replace_na(nodesel, "OSS")) %>% 
  filter(instance %in% instancesNCSolved)

ggplot(aes(x = pctCpu, y = pct, linetype = nodesel), data = dat1) + 
  geom_smooth(color = "black", se = F, size = 0.5) + 
  # facet_grid(rows = vars(pb)) +
  coord_cartesian(expand = FALSE, ylim = c(0, 1), xlim = c(0, 1)) +
  # ggtitle(str_c("Number of instances solved within a given cpu time")) +
  # labs(linetype = "") +
  scale_linetype_nodesel_varsel +
  theme_publish() + ylab("% of non-dominated points") + xlab("% of cpu time")
```

```{r, eval=FALSE}
ggplot(aes(x = pctCpu, y = pct, linetype = nodesel), data = dat1) + 
  geom_smooth(color = "black", se = F, size = 0.5) + 
  facet_grid(rows = vars(pb)) +
  coord_cartesian(expand = FALSE, ylim = c(0, 1), xlim = c(0, 1)) +
  # ggtitle(str_c("Number of instances solved within a given cpu time")) +
  labs(linetype = "") +
  scale_linetype_nodesel_varsel + 
  theme_publish() + ylab("% of non-dominated points") + xlab("% of cpu time")
```

```{r, eval=FALSE}
dat1 %>% 
  group_by(nodesel, OB) %>% 
  summarize(avg = mean(depth), std = sd(depth)) 
# %>% 
#   group_by(nodesel, OB) %>% 
#   summarize(avg = mean(avg), std = sd(std)) 
```

A possible explanation is that with a depth first strategy, the branch and bound algorithm tends to focus on specific parts of the objective space one by one. Using a breadth first strategy offer a better coverage of all the objective space earlier and thus, yields better upper bound sets earlier. This is confirmed if we take a look at Figure \@ref(fig:perfPlotND) where all the instances solved to optimality with no objective branching (`N`) or using a cone (`C`) have been grouped based on node selection strategy. Observe that using a breath first strategy (`B`) in general finds an approximation of the non-dominated set relatively faster, i.e. we obtain a better upper bound set. Since the figure reports relative numbers, it could be the case that a depth first strategy still is better if the cpu time is much better than a breath first strategy. However note that using a a breath first node selection strategy in general finds an optimal solution faster than a depth first node selection strategy (see Figure \@ref(fig:perfPlotPct)).  

One can observe in Figure \@ref(fig:perfPlotND) that the curve that represent the number of non-dominated points found for breadth first strategies tends to grow very quickly in the begining and slowly flattens in the middle, to finally grow fast again towards the end. In the begining, the first layers of the tree are explored. As the problems in these layers are less constrained and the upper bound set is relatively empty, many new non-dominated points are found. When the branch and bound reaches deeper layers, towards the middle, the problems becomes more constrained, which generate less new non-dominated points. Towards the end, the deepest parts of the branch and bound tree are explored. The problems here are so constrained that they become much easier to solve (at most one feasible solution at layer `n` + 1, the deepest possible layer in the tree) and thus, are proceeded much faster. In addition to that, the solutions found deep in the tree are more likely to be integer and thus to generate new non-dominated points. This leads to the acceleration of the number of non-dominated points found near the end of the algorithm. This observation also suggests that it may be interesting to combine breadth and depth first strategies to reach quickly the parts of the tree that generate a lot of non-dominated points.

# Comparing the branch and bound algorithm with an objective space search algorithm

```{r}
dat1 <- datOSS %>% 
  mutate(algConfig = "OSS", 
         pb = str_replace(instance, "Forget20-(.*?)_.*", "\\1"),
         tpstotal = cpu, alg = "OSS")
dat2 <- datAll %>% 
  filter (OB == "C" | OB == "N", instance %in% dat1$instance) %>% 
  mutate(alg = "BB")
dat <- bind_rows(dat2, dat1) %>% 
  group_by(instance) %>% 
  mutate(n = first(n)) %>% 
  group_by(algConfig, pb) %>%
  arrange(tpstotal) %>%
  mutate(count = row_number(), total = n()) %>% 
  group_by(pb) %>% 
  mutate(total = max(total)) %>% 
  group_by(pb, algConfig, tpstotal) %>% 
  arrange(pb, algConfig, tpstotal, count) %>% 
  filter(tpstotal < 1800 | row_number() == 1) %>% 
  mutate(pct = count/total) %>% 
  select(pb, algConfig, tpstotal, count, pct, total, n) %>% 
  separate(algConfig, into = c("nodeselVarsel", "OB"), sep = "\\|", remove = F) %>% 
  mutate(OB = replace_na(OB, "OSS"))

datWinner <- dat2 %>% 
  group_by(instance) %>% 
  nest() %>% 
  mutate(winner = map(data, function(df) {df %>% arrange(tpstotal) %>% slice(1)})) %>% 
  select(-data) %>% 
  unnest(c(winner))
  
tmp <- inner_join(dat1, datWinner, by = "instance") %>% 
  select(instance, n, pb.x, nbnodes, nbit, nbLB, algConfig.y, alg.x, alg.y, tpstotal.x, tpstotal.y) %>% 
  mutate(fct = nbLB/nbit, pctCpu = 100 * (tpstotal.y/tpstotal.x - 1), diff = tpstotal.y - tpstotal.x, fct1 = nbnodes/nbit) #%>% 
  # group_by(pb.x) %>% 
  # summarise("nbLB/nbit" = mean(fct), pctCpu = mean(pctCpu)) 
# tmp %>% kable() %>% kable_styling()

mean(tmp$fct1)
# mean(tmp$fct)
# mean(tmp$pctCpu)
# mean(tmp$diff)
```


```{r perfPlotPctOss, fig.asp=1.2, fig.cap="Performance profile: Number of instances in percent solved given cpu time. An instance is considered as unsolved if the cpu time exceed 1800 seconds (time limit)."}
ggplot(dat) +
  geom_step(aes(y=pct, x=tpstotal, color = OB, linetype = nodeselVarsel), alpha = 0.75) + 
  facet_grid(rows = vars(pb)) +
  scale_color_ob + scale_linetype_nodesel_varsel +
  theme_publish() + xlab("cpu (seconds)") + ylab("%") + 
  coord_cartesian(expand = FALSE, ylim = c(0, NA), xlim = c(-10, 1797)) 
```

```{r, eval=FALSE}
plt <- ggplot(dat, aes(y = tpstotal , x = n, color = algConfig)) +
    stat_summary(fun=mean, geom="line") +
    geom_jitter(width = 0.5, height = 0, size = 0.7) +
    facet_grid(cols = vars(pb), scales = "free") +
    ggtitle(str_c("Cpu time")) +
    theme_publish() +
    coord_cartesian(ylim = c(0, 1800)) 
ggplotly(plt)
```

```{r}
dat2 <- dat2 %>% 
  group_by(instance) %>% 
  nest() %>% 
  mutate(winner = map(data, function(df) {df %>% arrange(tpstotal) %>% slice(1)})) %>% 
  select(-data) %>% 
  unnest(c(winner)) %>% 
  mutate(alg = "BB")
dat <- bind_rows(dat2, dat1) 
  
```

The performance of the branch and bound (B&B) algorithm compared to the Objective Space Search (OSS) algorithm is shown in Figure \@ref(fig:perfPlotPctOss). In general the OSS algorithm performs better than the B&B algorithm (on average `r round(mean(tmp$diff))` seconds faster). For KP, the OSS algorithm performs much better than the current best branch and bound configuration. It is the case as well for UFLP, but the gap is smaller. Regarding AP, the gap is even smaller. A possible explanation is that again, as constraints are added to the initial problem, the totally unimodularity of the constraint matrix is lost, which leads to harder problems (harder than a single-objective AP) for the MIP solver, while the branch and bound without objective branching benefits from this property.

It is expected that OSS performs better since the OSS algorithm benefits from the power of single-objective MIP of solvers which has been improved over decades. The purpose of this paper is, to extend the concept of B&B from the bi-objective case to the tri-objective case and to investigate the behavior of such an algorithm. Thus, the scope is not to create an algorithm that outperforms all other algorithms for the tri-objective case. However, we do find it important to compare statistics from the branch and bound algorithm to statistics from the objective space search algorithm, as these are known to perform well on tri-objective combinatorial problems. 

Note that the B&B algorithm only maintains a single branching tree in contrast to the OSS algorithm that create a new branching tree for each MIP solved. Hence if the average computation in each node of the B&B algorithm can be reduced then B&B may be competitive with OSS. On average the number of nodes in the branching tree compared to the number of MIPs solved are a factor of approx. `r round(mean(tmp$fct1))` times larger. That is, processing a node in the brancing tree should on average be approx.`r round(mean(tmp$fct1))` times lower for the B&B algorithm to be competitive.
In this implementation, processing a node takes currently [stat1] times less cpu time than solving a MIP, which means that in order to be competitive, processing a node should take [stat2] times less time than it does now. We think that closing this gap is possible with further research in pruning nodes and finding lower bound sets faster (i.e. processing nodes faster) or by leading extensive experiments on alternative node and variable selection rules (i.e. reducing the size of the tree and thus, decreasing this factor of `r round(mean(tmp$fct1))`).

[to LRN: see piece of text just above for meaning of stat1 and stat2]

[to LRN: stat1 = (Avg cpu time per MIP) / (Avg cpu time per node)]

[to LRN: stat2 = (avg cpu time per node)/(competitive time) where competitive time = (avg cpu time per MIP)/`r round(mean(tmp$fct1))`; competitive time being the time processing a node should take in order to be competitive against OSS]

Furthermore, Figure \@ref(fig:perfPlotND) suggests that when the branch and bound approach becomes computationally competitive with the OSS approach, prematurely and early terminating the algorithm seems to lead to a better approximation of the pareto front for the B&B than for this OSS.










<!-- # Things that are currently not used -->

<!-- Reverse performance plot for the different algorithm configurations are given in Figure \@ref(fig:revPerfPlot). -->

```{r revPerfPlot, fig.cap="Reverse performance plots for the different problem classes", eval=FALSE}
ggplot(datAll %>% 
       group_by(algConfig, pb, n, solved, nodeselVarsel, OB) %>% 
         summarise(count = n()) %>% 
         group_by(algConfig, pb, n, nodeselVarsel, OB) %>% 
         summarise(solv = ifelse(length(count[solved==1]) == 0, 0, count[solved==1]), pct = solv/sum(count))) +
  # geom_col(aes(x=n, y=pct, fill = algConfig, linetype = OB), position = "dodge") +
  geom_step(aes(x=n, y=pct, color = nodeselVarsel, linetype = OB), alpha = 0.75) +
  geom_point(aes(x=n, y=pct, color = nodeselVarsel)) + 
  facet_grid(cols = vars(pb), scales = "free") +
  ggtitle(str_c("Number of instances solved within the time limit given n")) +
  labs(color = "nS | vS:", linetype = "oB:") +
  theme_publish() + #xlab("cpu") +
  scale_color_nodesel_varsel + scale_linetype_ob
```

```{r, fig.asp=1, layout="l-screen-inset", eval=FALSE}
plotCPUFacet <- function() {
  datP <- datAll
  tmp <- datP %>% group_by(coef, rangeC) %>% summarize(avg = mean(tpstotal))
  plt <- ggplot(datP, aes(y = tpstotal , x = n, color = algConfig)) +
    stat_summary(fun=mean, geom="line") +
    geom_jitter(width = 0.5, height = 0, size = 0.7) +
    facet_grid(cols = vars(pb), scales = "free") +
    ggtitle(str_c("Cpu time (note scale free)")) +
    theme_publish() +
    coord_cartesian(ylim = c(0, 1800)) +
    scale_color_algConfig
  plt
}
ggplotly(plotCPUFacet())
```

```{r, layout="l-screen-inset", eval=FALSE}
# datBestCpu <- 
#   datAllConfigs %>% 
#   group_by(algConfig) %>% 
#   summarise(cpu = mean(tpstotal), cpuMax = max(tpstotal), solved = if_else(mean(solved) == 1, T, F)) %>% 
#   arrange(cpu)
# winSeq <- datBestCpu$algConfig
winSeq <- c(winSq, algConfigs[!(algConfigs %in% winSq)])
winAlgConfigSeq <- winSeq
winSeq <- c(str_c("cpu_", winSeq), str_c("ctWin_", winSeq), str_c("nodes_", winSeq), str_c("prune_", winSeq))
winSeq <- winSeq[as.vector(sapply(1:algConfigsN, FUN = function(i) {c(i, i+algConfigsN, i+2*algConfigsN, i+3*algConfigsN)}))]

tmp <- datAll %>% 
  group_by(instance) %>%
  filter(n() == algConfigsN) %>%
  ungroup() 
tmpWin <- datWin %>% 
  filter(instance %in% unique(tmp$instance))


getResGroup <- function(dat, win, ...) {
  datYN <- dat %>% 
    group_by(instance, ...) %>% 
    summarise(YN = max(YN), YNse = max(YNse), YNs = max(YNs)) %>% 
    group_by(...) %>% 
    summarise(YN = mean(YN), YNse = mean(YNse), YNs = mean(YNs)) 
  
  datWinner <- win %>% 
    group_by(..., algConfig) %>%
    summarise(ctWin = n()) %>% 
    full_join(dat %>% distinct(..., algConfig)) %>% 
    replace_na(list(ctWin = 0))

  datResults <- dat %>% 
    group_by(..., algConfig) %>% 
    summarise(ct = n(), cpu = mean(tpstotal), cpuMax = max(tpstotal), cpuMin = min(tpstotal), 
              nodes = mean(nbnodes), nInf = mean(pctinfeas), nOpt = mean(pctopt), nDom = mean(pctdomi),
              dptLeaf = mean(avgdepthT), dptMinLeaf = mean(mindepthT), dptMaxLeaf = mean(maxdepthT),
              solved = min(solved)) %>% 
    full_join(datYN) %>% 
    full_join(datWinner) %>% 
    mutate(cpu = if_else(cpu == min(cpu), str_c(round(cpu, 1), "!"), str_c(round(cpu, 1))),
           ctWin = if_else(ctWin == max(ctWin), str_c(round(ctWin, 0), "!"), str_c(round(ctWin, 0))),
           nodes = if_else(nodes == min(nodes), str_c(round(nodes, 0), "!"), str_c(round(nodes, 0))),) %>% 
    ungroup() %>% 
    mutate(cpu = if_else(solved == 1, 
                         str_c(cpu, " [", round(cpuMin,1), ",", round(cpuMax,1), "]"), 
                         str_c(cpu, "*", " [", round(cpuMin,1), ",", round(cpuMax,1), "]")),
           YN = str_c(round(YN), " (", round(100*YNse/YN), "/", round(100*(YNs-YNse)/YN), "/", round(100*(YN-YNs)/YN), ")"),
           nodes = str_c(nodes, " [", round(dptMinLeaf), ",", round(dptLeaf), ",", round(dptMaxLeaf), "]"),
           prune = str_c("[", round(nInf), ",", round(nOpt), ",", round(nDom), "]")
           ) %>%
    select(-solved, -cpuMin, -cpuMax, -dptMinLeaf, -dptMaxLeaf, -dptLeaf, -nInf, -nOpt, -nDom) %>%
    pivot_wider(names_from = c(algConfig), values_from = c(cpu, nodes, prune, ctWin)) %>%
    select(..., ct, YN, !!winSeq)
  return(datResults)
}
datResRows <- getResGroup(tmp, tmpWin, pb, n)
tabResults <- NULL
for (p in unique(datResRows$pb)) {
  dat1 <- datResRows %>% filter(pb == p)
  dat2 <- getResGroup(tmp, tmpWin, pb) %>% filter(pb == p)
  tabResults  <- bind_rows(tabResults, dat1, dat2)
}

# 
# datYN <- datAll %>% 
#   group_by(instance, pb, n, coef) %>% 
#   summarise(YN = max(YN), YNse = max(YNse), YNs = max(YNs)) %>% 
#   group_by(pb, n, coef) %>% 
#   summarise(YN = mean(YN), YNse = mean(YNse), YNs = mean(YNs)) 
# 
# winSeq <- datWin %>% group_by(algConfig) %>% summarise(cpu = mean(tpstotal)) %>% arrange(cpu) %>% pull(algConfig)
# winSeq <- c(winSeq, algConfigs[!(algConfigs %in% winSeq)])
# winAlgConfigSeq <- winSeq
# winSeq <- c(str_c("cpu_", winSeq), str_c("nodes_", winSeq), str_c("prune_", winSeq))
# winSeq <- winSeq[as.vector(sapply(1:12, FUN = function(i) {c(i,i+12,i+24)}))]
# digits <- c(0,0,rep(c(1,0),12))
# tabResults <- datAll %>% 
#   group_by(pb, n, coef, algConfig) %>% 
#   summarise(cpu = mean(tpstotal), cpuMax = max(tpstotal), cpuMin = min(tpstotal), 
#             nodes = mean(nbnodes), nInf = mean(pctinfeas), nOpt = mean(pctopt), nDom = mean(pctdomi),
#             dptLeaf = mean(avgdepthT), dptMinLeaf = mean(mindepthT), dptMaxLeaf = mean(maxdepthT),
#             solved = min(solved)) %>% 
#   ungroup() %>% 
#   full_join(datYN) %>% 
#   mutate(cpu = if_else(solved == 1, 
#                        str_c(round(cpu, 1), " [", round(cpuMin,1), ",", round(cpuMax,1), "]"), 
#                        str_c(round(cpu, 1), "*", " [", round(cpuMin,1), ",", round(cpuMax,1), "]")),
#          YN = str_c(round(YN), " (", round(100*YNse/YN), "/", round(100*(YNs-YNse)/YN), "/", round(100*(YN-YNs)/YN), ")"),
#          nodes = str_c(round(nodes), " [", round(dptMinLeaf), ",", round(dptLeaf), ",", round(dptMaxLeaf), "]"),
#          prune = str_c("[", round(nInf), ",", round(nOpt), ",", round(nDom), "]")
#          ) %>%
#   select(-solved, -cpuMin, -cpuMax, -dptMinLeaf, -dptMaxLeaf, -dptLeaf, -nInf, -nOpt, -nDom) %>% 
#   pivot_wider(names_from = c(algConfig), values_from = c(cpu, nodes, prune)) %>% 
#   select(pb, n, YN, !!winSeq)

tabResults <- tabResults %>% 
  mutate_at(vars(contains(c("cpu", "win", "nodes"))), ~cell_spec(., bold = if_else(str_detect(., fixed("!")), T, F))) %>% 
  mutate_if(is.character, str_replace_all, pattern = "!", replacement = "")
digits <- c(0,0,0,rep(c(1,0,0,0),algConfigsN))
tabResults %>% 
  select(-pb) %>% 
  kable(
    digits = digits, 
    escape = F,
    col.names = c("n", "#", "YN (se, sne, us)",
                  rep(c("cpu", "win", "nodes (d leaf)", "prune (I/O/D)"), length(winAlgConfigSeq))),
    caption = "Results for each instance group.") %>% 
  kable_styling() %>% 
  add_header_above(c(" " = 3, setNames(rep(4, length(winAlgConfigSeq)), winAlgConfigSeq))) %>% 
  pack_rows("AP", min(which(tabResults$pb == "AP")), max(which(tabResults$pb == "AP"))) %>% 
  pack_rows("KP", min(which(tabResults$pb == "KP")), max(which(tabResults$pb == "KP"))) %>% 
  pack_rows("UFLP", min(which(tabResults$pb == "UFLP")), max(which(tabResults$pb == "UFLP"))) %>% 
  row_spec(c(max(which(tabResults$pb == "AP")), 
             max(which(tabResults$pb == "KP")), 
             max(which(tabResults$pb == "UFLP"))), italic = T, background = "lightgrey") %>% 
  scroll_box(width = "100%")
```

```{r, eval=FALSE}
datAll %>% 
  group_by(instance) %>%
  filter(n() == algConfigsN) %>%
  ungroup() %>% 
  group_by(n, pb, OB, nodeselVarsel) %>% 
  summarise(node = mean(nbnodes)) %>% 
  ggplot(aes(x = n, y = node, color = OB, linetype = nodeselVarsel)) + 
  geom_line(alpha = 0.75) +
  # geom_point(alpha = 0.75) +
  facet_grid(cols = vars(pb), scales = "free") + 
  ggtitle(str_c("Average branching tree size.")) +
  labs(color = "oB:", linetype = "nS:") +
  scale_color_ob + scale_linetype_nodesel_varsel +
  theme_publish() + xlab("n") + ylab("tree size") 
  # coord_cartesian(expand = FALSE, ylim = c(0, NA), xlim = c(-10, 1797)) 
```

