---
title: "Computational results for the instances"
author: "Nicolas Forget, Lars Relund Nielsen, Sune Lauth Gadegaard"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document: 
    df_print: paged
    number_sections: no
    toc: true
    toc_float: false
    theme: united #cosmo
    highlight: tango
    code_folding: show
    self_contained: true
editor_options: 
  chunk_output_type: console
---

<style type="text/css">
table {  /* Table  */
  font-size: 11px;
}
</style>




```{r setup, include=FALSE}
#' Function for loading missing packages that install them if not already installed.
#'
#' @param packages String vector with package names
#'
#' @return NULL (invisible)
#' @export
#'
#' @examples loadPackages(c("MASS", "ggplot2", "tikzDevice"))
loadPackages <- function(packages) {
  newP <- packages[!(packages %in% installed.packages()[,"Package"])]
  if(length(newP)) install.packages(newP, repos = "http://cran.rstudio.com/")
  lapply(packages, library, character.only = TRUE)
  invisible(NULL)
}
loadPackages(c("tidyverse", "knitr", "rgl", "gMOIP", "rmarkdown", "ggplot2", "plotly"))

library(knitr)
library(rgl)
if (isTRUE(getOption('knitr.in.progress'))) options(rgl.useNULL=TRUE)
rgl::setupKnitr()
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning=FALSE, message=FALSE, include = TRUE, 
  # cache = TRUE, autodep = TRUE, 
  echo=FALSE,
  out.width = "99%", fig.width = 8, fig.align = "center", fig.asp = 0.7
)
knit_hooks$set(webgl = hook_webgl, rgl = hook_rgl)
```


```{r, include=FALSE, eval=FALSE}
# Generation of stat for preliminary tests

  # Table for avg times

    gen_table_times <- function(tab){
      
      t <- tab %>%
        group_by(pb,n,varsel,nodesel) %>%
          summarize(avg = mean(tpstotal))

      t <- pivot_wider(t,names_from = nodesel, values_from = avg)
      t <- pivot_wider(t,names_from = varsel, values_from = c("breadth","depth"))
      t
    }
    
  # Plot of avg times
    
    gen_plot_times <- function(tab){
      
      t <- tab %>%
        group_by(pb,n,varsel,nodesel) %>%
          summarize(avg = mean(tpstotal))
      t <- pivot_wider(t,names_from = nodesel, values_from = avg)
      t <- pivot_wider(t,names_from = varsel, values_from = c("breadth","depth"))
      
      t %>% ggplot( aes(x=n)) + geom_line(aes(y=breadth_mfavg,colour="breadth-mfavg")) + geom_line(aes(y=depth_mfavg,colour="depth_mfavg")) + geom_line(aes(y=breadth_mof,colour="breadth_mof")) + geom_line(aes(y=depth_mof,colour="depth_mof")) + ylab("average time (sec)") + xlab("Number of variables") + ggtitle("Average computational time")
    }
    
  # Tables for nb nodes
    
    gen_table_nodes <- function(tab){
      
      t <- tab %>%
        group_by(pb,n,varsel,nodesel) %>%
          summarize(avg = mean(nbnodes))

      t <- pivot_wider(t,names_from = nodesel, values_from = avg)
      t <- pivot_wider(t,names_from = varsel, values_from = c("breadth","depth"))
      t
    }

  # Plot for nb nodes
    
    gen_plot_nodes <- function(tab){
      
      t <- tab %>%
        group_by(pb,n,varsel,nodesel) %>%
          summarize(avg = mean(nbnodes))
      t <- pivot_wider(t,names_from = nodesel, values_from = avg)
      t <- pivot_wider(t,names_from = varsel, values_from = c("breadth","depth"))
      
      t %>% ggplot( aes(x=n)) + geom_line(aes(y=breadth_mfavg,colour="breadth-mfavg")) + geom_line(aes(y=depth_mfavg,colour="depth_mfavg")) + geom_line(aes(y=breadth_mof,colour="breadth_mof")) + geom_line(aes(y=depth_mof,colour="depth_mof")) + ylab("Number of nodes") + xlab("Number of variables") + ggtitle("Average number of nodes explored")
    }
    
# Coefficient analysis
    
    # Plot relation Cn - |YN|
    
    gen_plot_correl_coef <- function(dat,pbb){
      
      datcoef <- dplyr::filter(dat,solved==1,pb==pbb)
      datcoef <- distinct(datcoef,instance,.keep_all = TRUE)

      datcoef %>% ggplot( aes(x=YN , y=ratioNDcoef, colour=n)) + geom_point() +     ggtitle("Relation between the number of non-dominated coefficients and |YN|")
    }
    
    # Plot |YN| & CPU time
    
    gen_plot_YN_cpu <- function(dat,pbb,nodemeth,varmeth,obmeth){
      
      datcoef <- dplyr::filter(dat,solved==1,pb==pbb,nodesel==nodemeth,varsel==varmeth,OB==obmeth)
      #datcoef <- distinct(datcoef,instance,.keep_all = TRUE)

      datcoef %>% ggplot( aes(x=YN , y=tpstotal, colour=n)) + geom_point() +     ggtitle("Relation between |YN| and the cpu time")
    }
    
    # Plot complexity per range
    
    gen_plot_range <- function(dat,pbb,coeff){
      
      datcoef <- dplyr::filter(dat,solved==1,pb==pbb,coef==coeff)
      datcoef <- distinct(datcoef,instance,.keep_all = TRUE)
      
      datcoef <- datcoef %>% group_by(n,rangemin,rangemax) %>% summarise(avg = mean(YN))
      plt <- datcoef %>% ggplot(aes(x=n , y=avg, group=interaction(rangemin,rangemax) , colour=interaction(rangemin,rangemax))) + geom_line() + xlab("Number of variables") + ylab("Average size of YN") + ggtitle(paste("Tri-objective",pbb,"problems with",coeff,"generation for the coefficients"))
      show(plt)
    }
    
    gen_plot_range_full <- function(dat,pbb,coeff){
      
      datcoef <- dplyr::filter(dat,solved==1,pb==pbb,coef==coeff)
      datcoef <- distinct(datcoef,instance,.keep_all = TRUE)
      
      datcoef %>% ggplot(aes(x=n, y=YN, group=interaction(rangemin,rangemax), colour=interaction(rangemin,rangemax))) + geom_point() + xlab("Number of variables") + ylab("Average size of YN") + ggtitle(paste("Tri-objective",pbb,"problems with",coeff,"generation for the coefficients"))
    }
    
    # Plot complexity per coef
    
    gen_plot_coef <- function(dat,pbb,rmin,rmax){
      
      datcoef <- dplyr::filter(dat, solved==1, pb==pbb, rangemin==rmin, rangemax==rmax)
      datcoef <- distinct(datcoef,instance,.keep_all = TRUE)
      
      datcoef <- datcoef %>% group_by(n,coef) %>% summarise(avg = mean(YN))
      plt <- datcoef %>% ggplot(aes(x=n , y=avg, group=coef , colour=coef)) + geom_line() + xlab("Number of variables") + ylab("Average size of YN") +  ggtitle(paste("Tri-objective",pbb,"problems with coefficients in the range [",rmin,",",rmax,"]"))
      show(plt)
    }

```

This report present the computational results for the instances. All instances are minimized, i.e. if an objective function $z(x)$ is maximized, we minimise $-z(x)$ instead. Note that in all instances only binary variables are considered.

# Instances

```{r load instance results}
dat <- read_csv("../statistics.csv") %>%   #"../convert/data/stat.csv"
  mutate(YNsRatio = YNs/YN, YNusRatio = 1-YNs/YN, YNsneRatio = (YNs-YNse)/YN)


datInput <- dat %>% 
  distinct(instance, .keep_all = TRUE) %>% 
  select(instance, pb, n, p, coef, rangeC, ratioNDcoef)
```

We consider a total of `r nrow(datInput)` instances named 

```
Forget20_[problemClass]_[n]_[p]_[rangeOfCosts]_[costGenerationMethod]_[constaintId]_[id].raw
``` 

where 

   - `problemClass` is either KP (knapsack problem), AP (assignment problem) or UFLP 
      (uncapacitated facility location problem) 
   - `n` is the size of the problem. 
   - `p` is the number of objectives.
   - `rangeOfCosts`: Objective coefficient range e.g. 1-1000.
   - `costGenerationMethod`: Either random, spheredown, sphereup or 2box (see below). 
   - `constaintId`: Same id if constraints are the same.
   - `id`: Instance id running within the constraint id.

The cost generation method for generating the coefficients of the objective functions are (given in column `coef`):

* `random`: the coefficient are generated randomly in the inerval [a,b].
* `spheredown`: the coefficient are generated on the non-dominated part of a sphere in minimisation in the interval [a,b].
* `sphereup`: the coefficient are generated on the non-dominated part of a sphere in maximisation in the interval [a,b].
* `2box`: the coefficients are generated randomly in two small boxes of R^3.

For further details see the documentation function `genSample` in the R package
[gMOIP](https://CRAN.R-project.org/package=gMOIP).

More detailed results for each instance can be seen in the files named `[instanceName]_results.html` ([see for instance this example](instance.html)).

# Input and output statistics

A set of statistics are stored for each instance and algorithm run. The statistics are stored in a [json result file (v1.0)](https://github.com/MCDMSociety/MOrepo/blob/master/contribute.md) for each instance and algorithm configuration named `[Instance name]_[Algorithm config]_result.json`. Most statistics are aggregated in file `stat.csv`.

## Input statistics

For each instance we have the following statistics:

```{r instance table, echo=FALSE}
links <- str_c("instances/", datInput$instance, ".html")
DT::datatable(
  datInput %>%
    mutate(instance = if_else(file.exists(links), str_c('<a href="', links, '">', instance, '</a>'), instance)),
  escape = F,
  caption = "Instances. A link to the results for the instance are given for some instances."
)
```

We have:

  - `pb`: Problem class 
    * KP = Knapsack Problem
    * AP = Assignment Problem
    * UFLP = Uncapacited Facility Location Problem
  - `n`: Number of variables.
  - `p`: Number of objectives.
  - `rangeC`: the range the objective coefficients are generated within. For the facility location problems, two sets of costs are generated: the assignment costs ($c_{ij}$) and the facility opening costs ($f_j$). It is reasonable to assume that these two sets of costs may not take their values in the same range. In this case `rangeC` defines two ranges: the assignment costs in $[a,b]$ and the facility opening costs which will be divided into two categories of instances:
    * facility location easy: $f_j \in [b+a,b+b]$. It is called "easy" because it seems that having high facility opening costs makes the problem easier.
    * facility location hard: $f_j \in [0.1a,0.1b]$. It is called "hard" because it seems that having small facility opening costs makes the problem harder.
  - `coef`: The method used for the generation of the objective coefficients (see section Research questions). We have:
    * random = random coefficient generation
    * 2box = two-boxes generation
    * spheredown = generation on the lower part of a sphere (in minimisation)
    * sphereup = generation on the upper part of a sphere (in minimisation)
  - `ratioNDcoef`: Proportion of objective coefficient vectors (here considered as a vector in $\mathbb{R}^p$, one for each variable, defining the objective coefficient for all the objective for this variable) that are non-dominated (with respect to the other objective coefficient vectors). Examples of the four ways of generating the objective coefficients are given:
  
**Random**

The coefficient are generated randomly with a uniform distribution in the range $[a,b]$. For three objectives, random generated coefficients looks as follows.

```{r, webgl = TRUE, echo = FALSE}
range <- matrix(c(1,100, 50,100, 10,50), ncol = 2, byrow = TRUE )
ini3D()
pts <- genSample(3, 1000, range = range, random = TRUE)
plotPoints3D(pts)
finalize3D()
```

**Sphere down**

The coefficients are generated on the lower part of a sphere (see next picture). Note that the sphere is adjusted such that the coefficients are in the range $[a,b]$, i.e. the sphere is not necessarily included in $[a,b]^p$.

```{r, webgl = TRUE, echo = FALSE, cache=FALSE}
cent <- c(1000,1000,1000)
r <- 750
planeC <- c(cent-r/3)
planeC <- c(planeC, -sum(planeC^2))
pts <- genSample(3, 500,
  argsSphere = list(center = cent, radius = r, below = TRUE, plane = planeC, factor = 6))
ini3D()
rgl::spheres3d(cent, radius=r, color = "grey100", alpha=0.1)
plotPoints3D(pts)
rgl::planes3d(planeC[1],planeC[2],planeC[3],planeC[4], alpha = 0.5, col = "red")
finalize3D()
```

**Sphere up**

The coefficients are generated on the upper part of a sphere (see next picture). Note that the sphere is adjusted such that the coefficients are in the range $[a,b]$, i.e. the sphere is not necessarily included in $[a,b]^p$.

```{r, webgl = TRUE, echo = FALSE, cache=FALSE}
cent <- c(1000,1000,1000)
r <- 750
planeC <- c(cent+r/3)
planeC <- c(planeC, -sum(planeC^2))
pts <- genSample(3, 500,
  argsSphere = list(center = cent, radius = r, below = FALSE, plane = planeC, factor = 6))
ini3D()
rgl::spheres3d(cent, radius=r, color = "grey100", alpha=0.1)
plotPoints3D(pts)
rgl::planes3d(planeC[1],planeC[2],planeC[3],planeC[4], alpha = 0.5, col = "red")
finalize3D()
```

**2box**

The coefficients are generated randomly but in two specific parts of $[a,b]^p$ (see next picture).

```{r, webgl = TRUE, echo = FALSE, cache=FALSE}
range <- matrix(c(1,1000, 1,1000, 1,1000), ncol = 2, byrow = TRUE )
ini3D(argsPlot3d = list(box = TRUE, axes = TRUE))
pts <- genSample(3, 300, range = range, box = TRUE)
plotPoints3D(pts)
finalize3D()
```


## Algorithm configuration

The algorithm is configurated using: 

```{r instance config}
dat %>% 
  select(instance, nodesel:OB)
```

The branch and bound algorithm will always use the linear relaxation as lower bound set and the incumbent set as upper bound set. At each nodes, the extreme points of the lower bound set are checked for integrality and possibly added to the upper bound set. Different configurations are bases on:

  - `nodesel`: node selection strategy.
  - `varsel`: variable selection strategy.
  - `OB`: objective branching strategy.

Combinations of `nodesel`, `varsel` and `OB` gives the configuration of the algorithm:

* Node selection: in the multi-objective branch and bound literature, depth first strategies are (almost) always used and are considered as better strategies than breadth first. However, if a problem with an "easy" single-objective version is solved, then many non-dominated points may be found in a node close to the root node, in the early stages of the tree. Hence, using a breadth first strategy may provide a better upper bound set earlier in the algorithm, as a larger number of points are expected to be found shallow in the tree while only a few points can be found at each node deep in the tree. 

* Variable selection: to the best of our knowledge, no extensive study for variable selection has been conducted in the literature. Sometimes, this component is even not mentioned. As it is not the main purpose of this study, only two rules that rely on the caracteristics of the lower bound set will be tested here.

  + mof: Most Often Fractional. The branching is operated on the variable that is the most often fractional among the extreme points of the lower bound set.
  
  + mfavg: Most Fractional in AVeraGe. The branching is operated on the variable such that its average value among the extreme points of the lower bound set is the most fractional, i.e. the closest to 0.5.
  
* Branching scheme: in a regular branch and bound, branching is operated (i.e. sub-problems are created) in the decision space. In the bi-objective case, it has been shown that creating additional sub-problems in the objective space (procedure called objective branching in this study) leads to better computational times. Three versions of the branch and bound will be tested:

  + None: no objective branching is performed. Hence, this version is just a regular branch and bound.
  
  + exact: objective branching is performed using the algorithm from [master thesis paper].
  
  + unique cone: a unique sub-problem is created at each node. In this case, objective branching is applied on the nadir point of the local upper bounds dominated by the lower bound set. See [master thesis paper] for more details.


## Output statistics

For each algorithm run we have the following statistics.

```{r instance output}
datOutput <- dat %>% 
  select(instance, solved:maxnbpbOB)
datOutput
```

  - `solved`: 1 if the instance is solved within 3600 sec, 0 otherwise.
  - `YN`: size of YN. If `solved` = 0, it represent the size of the upper bound set at 3600 sec, when the algorithm stops.
  - `nbnodes`: number of nodes explored.
  - `mindepthT`: minimal depth of a leaf node.
  - `maxdepthT`: maximal depth of a leaf node (and thus of the tree).
  - `avgdepthT`: average depth of the leaf nodes.
  - `avgdepthYN`: average depth of the nodes where the non-dominated points were found.
  - `nbleaf`: number of leaf nodes.
  - `nbinfeas`: number of nodes pruned by infeasibility.
  - `pctinfeas`: proportion (in %) of leaf nodes pruned by infeasibility.
  - `tpsinfeas`: average time spend to prune a node by infeasibility (in msec).
  - `nbopt`: number of nodes pruned by optimality.
  - `pctopt`: proportion (in %) of leaf nodes pruned by optimality.
  - `tpsopt`: average time spend to prune a node by optimality (in msec).
  - `nbdomi`: number of nodes pruned by dominance.
  - `pctdomi`: proportion (in %) of leaf nodes pruned by dominance.
  - `avgdomi`: average time spend to prune a node by dominance (in msec).
  - `nbLB`: number of lower bound set computed.
  - `avgfacets`: average number of facets in the lower bound set (i.e. in $\mathcal{L} + \mathbb{R}^p$).
  - `avgNDf`: average number of strictly non-dominated facets.
  - `pctavgNDf`: proportion (in %) of facets that are strictly non-dominated.
  - `avgWNDf`: average number of weekly non-dominated facets.
  - `pctavgWNDf`: proportion (in %) of facets that are weekly non-dominated.
  - `maxfacets`: maximal number of facets a lower bound set had in the tree.
  - `maxNDf`: number of strictly non-dominated facets in the lower bound set with the maximal number of facets.
  - `pctmaxNDf`: proportion (in %) of facets that are strictly non-dominated in the lower bound set with the maximal number of facets.
  - `maxWNDf`: number of weekly non-dominated facets in the lower bound set with the maximal number of facets.
  - `pctmaxWNDf`: proportion (in %) of facets that are weekly non-dominated in the lower bound set with the maximal number of facets.
  - `tpstotal`: CPU time (in sec) used to solve the instance. 3600 if the instance is not solved.
  - `tpsLB`: CPU time (in sec) used to compute lower bound sets.
  - `pcttpsLB`: proportion (in %) of the total CPU time spend in the computation of lower bound sets.
  - `tpsdomi`: CPU time (in sec) used to dominance test when the algorithm has to determine whether a node can be pruned by dominance or not.
  - `pcttpsdomi`: proportion (in %) of the total CPU time spend in the dominance test when the algorithm has to determine whether a node can be pruned by dominance or not.
  - `tpsUB`: CPU time (in sec) used to update the upper bound set.
  - `pcttpsUB`: proportion (in %) of the total CPU time spend in updating the upper bound set.
  - `tpsnodesel`: CPU time (in sec) used to choose the next node to develop.
  - `pcttpsnodesel`: proportion (in %) of the total CPU time spend in choosing the next node to develop.
  - `tpsvarsel`: CPU time (in sec) used to choose the variable to branch on.
  - `pcttpsvarsel`: proportion (in %) of the total CPU time spend in choosing the variable to branch on.
  - `tpsOB`: CPU time (in sec) used to create the sub-problems in the objective space, i.e. to compute objective branching. (/!\ it requires two different steps in total: computing the SLUBs but also do additional dominance test to determine the dominance status of each local upper bounds ! This number take into account the two steps.)
  - `pcttpsOB`: proportion (in %) of the total CPU time spend in computing objective branching.
  - `tpsSLUB`: CPU time (in sec) used to compute the super local upper bounds.
  - `pcttpsSLUB`: proportion (in %) of the total CPU time spend in computing the super local upper bounds.
  - `tpsdomiLUB`: CPU time (in sec) used to do the additionnal dominance tests to get the dominance status of EACH local upper bound.
  - `pcttpsdomiLUB`: proportion (in %) of the total CPU time spend in doing the additionnal dominance tests on the local upper bounds.
  - `nbOB`: number of nodes where two or more sub-problems are created in the objective space. When using the exact objective branching (`OB` = exact), it in particular shows how often it is actually possible to split the objective space with the definition of the sub-problems that we used ($z(x) \leqq \bar{z}$, $\bar{z} \in \mathbb{R}^p$).
  - `pctnbOB`: proportion (in %) of the nodes explored that where split in two or more sub-problems in the objective space.
  - `avgdepthOB`: [relevant only if `OB` = exact] average depth of the nodes split in two or more sub-problems in the objective space.
  - `mindepthOB`: [relevant only if `OB` = exact] minimal depth of the nodes split in two or more sub-problems in the objective space.
  - `maxdepthOB`: [relevant only if `OB` = exact] maximal depth of the nodes split in two or more sub-problems in the objective space.
  - `avgnbpbOB`: [relevant only if `OB` = exact] average number of sub-problems created in the objective space in the nodes split in two or more sub-problems in the objective space.
  - `maxnbpbOB`: [relevant only if `OB` = exact] average number of sub-problems created in the objective space in the nodes split in two or more sub-problems in the objective space.

For each instance we also store the non-dominated set $\mathcal{Y}_N$ found by the algorithm and if an exact solution was found the efficiet set $\mathcal{X}_E$. Statistics of how the non-domnated points are found are stored in `yNStat`. The statistics are the following:

  - the $p$ first columns correspond the values of the objective functions.
  - `node`: number of the node where this point has been discovered. The higher this number is, the later the point has been discovered.
  - `time`: time (in sec) elapsed between the start of the algorithm and when this point has been found (for the first time).
  - `depth`: depth of the node where this point has been found (for the first time).
  
`yNStat` are sorted in exactly the same order as $\mathcal{X}_E$, i.e. each row represent the non-dominated point and its corresponding solution.
  

# Detailed results for each instance

Detailed results for each instance can be generated using `instance.Rmd`. The report is already generated for some of the instances (see links in the table with input statistics). Some instances that might be of interest:

```{r}
tmp <- dat %>% group_by(instance) %>% summarise_at(vars(contains(c("YN", "total"))), list(mean = mean, sd = sd, max = max), na.rm = TRUE)
toLink <- function(inst) {
  links <- str_c('instances/', inst, '.html')
  if_else(file.exists(links), str_c('<a href="', links, '">', inst, '</a>'), inst)
}
```

  * Instances with lowest number of nondominated points: `r toLink(tmp %>% top_n(-3, YN_mean) %>% pull(instance))`
  * Instances with higest number of nondominated points: `r toLink(tmp %>% top_n(3, YN_mean) %>% pull(instance))`
  * Instances with lowest unsupported nondominated points percentage: `r toLink(tmp %>% top_n(-3, YNusRatio_mean)  %>% pull(instance))`
  * Instances with higest unsupported nondominated points percentage: `r toLink(tmp %>% top_n(3, YNusRatio_mean)  %>% pull(instance))`
  * Instances with higest supported non-extreme nondominated points percentage: `r toLink(tmp %>% top_n(3, YNsneRatio_mean) %>% pull(instance))`
  * Instances with higest variance in cpu time: `r toLink(tmp %>% top_n(3, tpstotal_sd) %>% pull(instance))`


# Research questions

We first consider 

  1. How do input statistics affect the number of nondominated solutions?
     a) Does the cost range have an effect?
     b) Does the generation method of the objective coefficients (`coef`) have an effect?
     c) Does the proportion of objective coefficient vectors that are non-dominated (`ratioNDcoef`) have an effect?
     d) Can we exclude some values of the input statistics for further study (e.g. some ranges)?
     
  2. Which instances are hard to solve?
     a) What is the cpu for each instance?
     b) Which sphere generation instances are hardest to solve?
     c) What is the std.dev. within each instance group?
     d) In which subprocedures are the cpu time used?

Next, we study how some components of the branch and bound influences the behaviour of the algorithm:

  2. How does depth and breadth first strategies affects the behaviour of the algorithm? That is,
  we consider `nodesel` for fixed `varsel` and `OB`.
      a) Are some problem classes solved best with one node selection strategy compared to others?
      b) Are 'easy' problems solved best with one node selection strategy compared to others?
      c) Does different `OB` strategies affect the node selection strategy?

The primary focus is on computational time and number of nodes explored the size of the brancing
tree before the algorithm ends.

  


<!-- The second purpose of this study is to learn how the caracteristics of an instance can affect its -->
<!-- difficulty. The difficulty will be here mesured by the *size of the non-dominated set*, and the -->
<!-- *computational time* required to solve the instance. The computational time will be determined -->
<!-- using the branch and bound algorithm previously described. Note that the complexity of an objective -->
<!-- space search algorithm is positively correlated to the size of the non-dominated set as the more -->
<!-- non-dominated there are, the more integer programs have to be solved. -->

<!-- The focus will be here on the objective function coefficients. The first research question here is: **does the range of the objective function coefficient has an impact on the difficulty of the problem ?**. In order to answer that, three different ranges will be tested: -->

<!-- * $[1,10]$: a small range of values. -->
<!-- * $[1,1000]$: a large range of values -->
<!-- * $[1000,2000]$: a large range of values, but shifted to larger values. -->



<!-- The second research question related to that topic is: **does the method used to generate the coefficients of the objective function has an impact on the difficulty of the problem ?**. Indeed, the usual procedure to generate objective function is to generate the coefficients randomly in a given range $[a,b]$. However, one could imagine other way to generate coefficient. Four methods will be studied here. -->


# Analysis: Research Question 1 (R1) - How do input statistics affect the number of nondominated solutions?

In this section, relations between |YN| (`YN`) and various input statistics. How do input
statistics affect the number of nondominated solutions?

## R1a: Does the cost range have an effect?

```{r}
datP <- dat %>%  
  dplyr::filter(solved==1) %>% 
  select(instance, pb, n, p, coef, rangeC, ratioNDcoef, YN) %>% 
  distinct() 

ggplot(datP, aes(y = YN , x = n)) + 
  geom_jitter(aes(color = rangeC)) + 
  facet_grid(rows = vars(coef), cols = vars(pb), margins = T, scales = "free") +
  ggtitle("Effect of ranges (|YN| for each instance)") +
  theme(legend.position="bottom", legend.title=element_blank()) 

datPA <- datP %>% 
  group_by(pb, coef, rangeC, n) %>% 
  dplyr::summarise(YN = mean(YN))

ggplot(datPA, aes(y = YN , x = n)) + 
  geom_jitter(aes(color = rangeC)) + 
  # geom_line(aes(color = str_c(pb, coef, rangemin, rangemax, sep ="-"))) +
  facet_grid(rows = vars(coef), cols = vars(pb), margins = T, scales = "free") +
  # guides(color=FALSE) +
  ggtitle("Effect of ranges (Average |YN| for each instance group)") +
  theme(legend.position="bottom", legend.title=element_blank()) 
```

Based on the plots we may answer the research question by concluding that range have an effect. A small cofficient range gives a smaller range of possible objective values and hence fewer possible nondominated points. Note a smaller range may result in a larger amount of alternative solutions corresponding to a nondominted point. The speed of the algorithm may be affected by alternative optimal solutions.

For KP and AP scaling the cost don't affect the size of nondominated points. For UFLP there is an effect due to the way costs are generated so here scaling don't give the same results (we have to choose a setting here).  [Missing comments for IP]

We may conclude that further study only consider range [1,1000] and the two cases for the UFLP with this range. 

Note that the number of nondominated points increase with the number of variables which is due to the total possible range of the objectives increase with `n` (there is room for more points). 

```{r}
plot_ly(x = ~rangeGapZ1, y = ~rangeGapZ2, z = ~rangeGapZ3, type="scatter3d",
        mode="markers",
        data = dat %>% dplyr::filter(pb == "AP"),
        marker = list(size=3),
        color = ~n) %>% plotly::layout(title = "AP")
plot_ly(x = ~rangeGapZ1, y = ~rangeGapZ2, z = ~rangeGapZ3, type="scatter3d",
        mode="markers",
        data = dat %>% dplyr::filter(pb == "KP"),
        marker = list(size=3),
        color = ~n) %>% plotly::layout(title = "KP")
plot_ly(x = ~rangeGapZ1, y = ~rangeGapZ2, z = ~rangeGapZ3, type="scatter3d",
        mode="markers",
        data = dat %>% dplyr::filter(pb == "UFLP"),
        marker = list(size=3),
        color = ~n) %>% plotly::layout(title = "UFLP")
```



## R1b: Does the generation method of the objective coefficients (`coef`) have an effect?

```{r, fig.asp = 1}
ggplot(datP %>% dplyr::filter(pb != "UFLP"), aes(y = YN , x = n)) +
  geom_jitter(aes(color = coef)) +
  facet_grid(rows = vars(rangeC), cols = vars(pb), margins = T, scales = "free") +
  ggtitle("Effect of generation method (|YN| for each instance)") +
  theme(legend.position="bottom", legend.title=element_blank())
ggplot(datP %>% dplyr::filter(pb == "UFLP"), aes(y = YN , x = n)) +
  geom_jitter(aes(color = coef)) +
  facet_grid(rows = vars(rangeC), cols = vars(pb), scales = "free") +
  ggtitle("Effect of generation method (|YN| for each instance)") +
  theme(legend.position="bottom", legend.title=element_blank())

datPA <- datP %>%
  group_by(pb, coef, rangeC, n) %>%
  dplyr::summarise(YN = mean(YN))
ggplot(datPA %>% dplyr::filter(pb != "UFLP"), aes(y = YN , x = n)) +
  geom_jitter(aes(color = coef)) +
  facet_grid(rows = vars(rangeC), cols = vars(pb), margins = T, scales = "free") +
  ggtitle("Effect of generation method (average |YN| for each instance group)") +
  theme(legend.position="bottom", legend.title=element_blank())
ggplot(datPA %>% dplyr::filter(pb == "UFLP"), aes(y = YN , x = n)) +
  geom_jitter(aes(color = coef)) +
  facet_grid(rows = vars(rangeC), cols = vars(pb), margins = F, scales = "free") +
  ggtitle("Effect of generation method (average |YN| for each instance group)") +
  theme(legend.position="bottom", legend.title=element_blank())
```

Based on the plots we may answer the research question by concluding that the objective cofficient generation method have a high effect on the number of solutions. Random generation on average produce the lowest amount, then 2box. The two sphere methods seems to give approx. the same results. 

Note in general the speed of the algorithm is higly correlated with the number of non-dominated points. That is, instances with a high number of nondominated points are harder to solve. Based on this we may conclude that using one of the sphere methods for further study seems a good choice. We may also consider comparing it against the easiest method.  


## R1c: Does the proportion of objective coefficient vectors that are non-dominated (`ratioNDcoef`) have an effect?

This is highly related to the objective cofficient generation method (and the range):

```{r}
tmp <- dat %>% group_by(pb, coef, rangeC) %>% summarize(ratio = mean(ratioNDcoef))
tmp %>% arrange(pb, rangeC, ratio)
```

Note for the fixed `pb` and `rangeC` the ratio approx. increase in the order random, 2box, spheredown and sphereup (with random and 2box (spheredown and sphereup) often close to each other). However it is important to keep in mind that the ratio is highly dependent on the problem class (how variables are linked together). The UFLP have much smaller ratios due to that the costs relate to different aspects of the problem. 


```{r, fig.asp = 1}
ggplot(datP %>% dplyr::filter(pb != "UFLP"), aes(y = YN , x = n)) +
  geom_jitter(aes(color = ratioNDcoef, shape = coef)) +
  facet_grid(rows = vars(rangeC), cols = vars(pb), margins = T, scales = "free") +
  ggtitle("Effect of ratio (|YN| for each instance)") +
  theme(legend.position="bottom", legend.title=element_blank())
ggplot(datP %>% dplyr::filter(pb == "UFLP"), aes(y = YN , x = n)) +
  geom_jitter(aes(color = ratioNDcoef, shape = coef)) +
  facet_grid(rows = vars(rangeC), cols = vars(pb), margins = F, scales = "free") +
  ggtitle("Effect of ratio (|YN| for each instance)") +
  theme(legend.position="bottom", legend.title=element_blank())
```

```{r, eval=FALSE}
ggplot(datP %>% dplyr::filter(pb != "UFLP"), aes(y = YN , x = n)) +
  geom_jitter(aes(color = ratioNDcoef, shape = rangeC)) +
  facet_grid(rows = vars(coef), cols = vars(pb), margins = T, scales = "free") +
  ggtitle("Effect of ratio (|YN| for each instance)") +
  theme(legend.position="bottom", legend.title=element_blank())
ggplot(datP %>% dplyr::filter(pb == "UFLP"), aes(y = YN , x = n)) +
  geom_jitter(aes(color = ratioNDcoef, shape = rangeC)) +
  facet_grid(rows = vars(coef), cols = vars(pb), margins = F, scales = "free") +
  ggtitle("Effect of ratio (|YN| for each instance)") +
  theme(legend.position="bottom", legend.title=element_blank())
```


Based on the plot we may answer the research question by concluding that the ratio may have a high effect on the number of non-dominated solutions. Given a fixed problem class and objective coefficient range, the ratio may have a high effect on the dificulty of the instance. Note however, that if problem class and range not is known you can not guess the number of ND points. 

## R1d: Can we exclude some values of the input statistics for further study (e.g. some ranges)?

The above analysis may conclude that instances with a lower number of nondominated points can be generated by

  1. Use a narrow cost range.
  2. Use random cost generation which result in a low ratio.
  
To have harder instances we may skip these instances and instead use instances with a higher number of nondominated points by

  1. Use a wide cost range (e.g. [1,1000]).
  2. Use a sphere generation method.
  
Scaling the range don't have a high effect on most problem classes so fix a cost range seems okay. If we want to compare agains easy problems we may use the first option too. 


# Analysis: Research Question 2 (R2) - Which instances are hard to solve?

We focus on cpu time (`tpstotal`) and try to get an overview over difficulty of the instances.

## R2a: What is the cpu for each instance?

First let us have a general look:

```{r}
ggplot(dat, aes(y = tpstotal , x = n)) + 
  stat_summary(fun=mean, geom="line", aes(color = interaction(nodesel,varsel,OB))) +
  geom_jitter(aes(color = interaction(nodesel,varsel,OB)), width = 1, height = 0, size = 0.7) +
  # facet_grid(rows = vars(coef), cols = vars(pb), margins = T, scales = "free") +
  ggtitle("Cpu time for all instances (lines are means)") +
  theme(legend.position="bottom", legend.title=element_blank()) 
```

The general picture is fuzzy due to different problem classes, ranges, generation methods and algorithm configurations. Let us have a closer look:

```{r, fig.asp=1}
datP <- dat %>% 
  dplyr::filter(pb == "AP") 
ggplot(datP, aes(y = tpstotal , x = n)) + 
  stat_summary(fun=mean, geom="line", aes(color = interaction(nodesel,varsel,OB))) +
  geom_jitter(aes(color = interaction(nodesel,varsel,OB)), width = 3, height = 0, size = 0.7) +
  facet_grid(rows = vars(coef), cols = vars(rangeC), margins = T, scales = "free") +
  ggtitle("Cpu time for AP (note scale free)") +
  theme(legend.position="bottom", legend.title=element_blank())
```


```{r, fig.asp=1}
datP <- dat %>% 
  dplyr::filter(pb == "KP") 
ggplot(datP, aes(y = tpstotal , x = n)) + 
  stat_summary(fun=mean, geom="line", aes(color = interaction(nodesel,varsel,OB))) +
  geom_jitter(aes(color = interaction(nodesel,varsel,OB)), width = 1, height = 0, size = 0.7) +
  facet_grid(rows = vars(coef), cols = vars(rangeC), margins = T, scales = "fixed") +
  ggtitle("Cpu time for KP") +
  theme(legend.position="bottom", legend.title=element_blank())
```


```{r, fig.asp=1}
datP <- dat %>% 
  dplyr::filter(pb == "UFLP") 
ggplot(datP, aes(y = tpstotal , x = n)) + 
  stat_summary(fun=mean, geom="line", aes(color = interaction(nodesel,varsel,OB))) +
  geom_jitter(aes(color = interaction(nodesel,varsel,OB)), width = 4, height = 0, size = 0.7) +
  facet_grid(rows = vars(coef), cols = vars(rangeC), margins = T, scales = "fixed") +
  ggtitle("Cpu time for UFLP") +
  theme(legend.position="bottom", legend.title=element_blank()) 
```

Some observations

  - There is no clear winner among the different algorithm configurations.
  - There seems to be a bigger difference for KP.
  - Hardest instances are the one generated with the sphere methods.

Let us find the winner configuration for each instance given some groupings:

```{r}
datP <- 
  dat %>% 
  group_by(instance) %>% 
  nest() %>% 
  mutate(data = map(data, function(df) {df %>% arrange(tpstotal) %>% slice(1)})) %>% 
  mutate(win = map(data, function(df) {str_c(df$nodesel[1], ".", df$varsel[1], ".", df$OB[1])})) %>% 
  unnest(c(win, data)) 

res <- datP %>% 
  mutate(count = nrow(.)) %>% 
  group_by(win) %>% summarize(pct = n()/mean(count)) %>% 
  arrange(desc(pct))
ggplot(res, aes(y = pct, x = win, fill = win)) + 
  # stat_summary(fun=mean, geom="line", aes(color = win)) +
  geom_col() + 
  ggtitle("Winner pct (all instances)") +
  theme(legend.position="bottom", legend.title=element_blank())
```

Overall breadth methods win.

```{r}
res <- datP %>% 
  group_by(pb) %>% 
  mutate(count = n()) %>% 
  group_by(pb, win) %>% summarize(pct = n()/mean(count)) %>% 
  arrange(pb,desc(pct))
ggplot(res, aes(y = pct, x = win, fill = win)) + 
  # stat_summary(fun=mean, geom="line", aes(color = win)) +
  geom_col() + 
  facet_grid(rows = vars(pb), scales = "fixed") +
  ggtitle("Winner pct") +
  theme(legend.position="bottom", legend.title=element_blank())
```

Note if we look at different problem classes different variable selection methods win. 

```{r}
res <- datP %>% 
  group_by(pb, coef) %>% 
  mutate(count = n()) %>% 
  group_by(pb, coef, win) %>% summarize(pct = n()/mean(count)) %>% 
  arrange(pb, coef, desc(pct))
ggplot(res, aes(y = pct, x = win, fill = win)) + 
  # stat_summary(fun=mean, geom="line", aes(color = win)) +
  geom_col() + 
  facet_grid(rows = vars(pb), cols = vars(coef), scales = "fixed") +
  ggtitle("Winner pct") +
  theme(legend.position="bottom", legend.title=element_blank())
```

If we group by `coef`, we have different winners. This may have something with the hardness of the instance. 

Will different methods be used for harder instances (here we use YN as a proxy for hard). Let us try to plot the aggegated percentage winner configuration:

```{r}
res <- datP %>% 
  group_by(pb, YN) %>% 
  mutate(count = n()) %>% 
  group_by(pb, YN, win) %>% summarize(pct = n()/mean(count)) %>% 
  arrange(pb, YN, desc(pct))
ggplot(res, aes(x = YN, fill = win)) + 
  geom_histogram(position = "fill", bins = 10) + 
  stat_bin(bins = 10, geom="text", colour="white", size=3.5,
            aes(label=..count.., group = win), position=position_fill(vjust=0.5)) +
  facet_grid(rows = vars(pb), margins = T, scales = "fixed") +
  ggtitle("Pct for each configuration given 10 bins") +
  theme(legend.position="bottom", legend.title=element_blank())
```

It can be seen that for some problems the best configuration change from easy problems to hard.

Finally we may plot only the winner cpu times for each instance:

```{r, fig.asp=1}
datPP <- datP %>% 
  dplyr::filter(pb == "AP") 
ggplot(datPP, aes(y = tpstotal , x = n)) + 
  # stat_summary(fun=mean, geom="line", aes(color = win)) +
  geom_jitter(aes(color = win), width = 3, height = 0, size = 0.7) +
  facet_grid(rows = vars(coef), cols = vars(rangeC), margins = T, scales = "fixed") +
  ggtitle("Cpu time for AP") +
  theme(legend.position="bottom", legend.title=element_blank())
```


```{r, fig.asp=1}
datPP <- datP %>% 
  dplyr::filter(pb == "KP") 
ggplot(datPP, aes(y = tpstotal , x = n)) + 
  # stat_summary(fun=mean, geom="line", aes(color = win)) +
  geom_jitter(aes(color = win), width = 1, height = 0, size = 0.7) +
  facet_grid(rows = vars(coef), cols = vars(rangeC), margins = T, scales = "fixed") +
  ggtitle("Cpu time for KP") +
  theme(legend.position="bottom", legend.title=element_blank())
```


```{r, fig.asp=1}
datPP <- datP %>% 
  dplyr::filter(pb == "UFLP") 
ggplot(datPP, aes(y = tpstotal , x = n)) + 
  # stat_summary(fun=mean, geom="line", aes(color = win)) +
  geom_jitter(aes(color = win), width = 4, height = 0, size = 0.7) +
  facet_grid(rows = vars(coef), cols = vars(rangeC), margins = T, scales = "fixed") +
  ggtitle("Cpu time for UFLP") +
  theme(legend.position="bottom", legend.title=element_blank()) 
```



## R2b: Which sphere generation instances are hardest to solve?

```{r, fig.asp=1}
res <- datP %>% 
  dplyr::filter(grepl("sphere", coef), grepl("[1,1000]", rangeC)) %>% 
  group_by(pb, coef) %>% 
  summarize(avgCpu = mean(tpstotal)) %>% 
  arrange(pb, desc(avgCpu))
res
# ggplot(res, aes(y = pct, x = win, fill = win)) + 
#   # stat_summary(fun=mean, geom="line", aes(color = win)) +
#   geom_col() + 
#   facet_grid(rows = vars(pb), cols = vars(coef), scales = "fixed") +
#   ggtitle("Winner pct") +
#   theme(legend.position="bottom", legend.title=element_blank())
# 
# 
# ggplot(datPP, aes(y = tpstotal , x = n)) + 
#   # stat_summary(fun=mean, geom="line", aes(color = win)) +
#   geom_jitter(aes(color = win), width = 4, height = 0, size = 0.7) +
#   facet_grid(rows = vars(coef), cols = vars(rangeC), scales = "fixed") +
#   ggtitle("Cpu time for UFLP") +
#   theme(legend.position="bottom", legend.title=element_blank()) 
```

WE NEED INFO ABOUT SUPPORTED VS UNSUPPORTED

## R2c: Is the cpu time the same within each instance group?

By instance group we mean an unique combination of `namePrefix`, `constId`, `pb`, `n`, `coef`, `rangeC`, `nodesel`, `varsel`, `OB`

```{r}
# datP <- dat %>% 
#   select(instance, namePrefix, insId, constId, pb, n, coef, rangeC, ratioNDcoef, nodesel, varsel, OB, YN, tpstotal) %>% #contains("tps")
#   mutate(iG = str_c(namePrefix, constId, pb, n, coef, rangeC, nodesel, varsel, OB, sep = "."))
datP <- dat %>% 
  select(instance, namePrefix, insId, constId, pb, n, coef, rangeC, ratioNDcoef, nodesel, varsel, OB, YN, tpstotal) %>% #contains("tps")
  group_by(namePrefix, constId, pb, n, coef, rangeC, nodesel, varsel, OB) %>% nest()

datPSd <- datP %>% 
  mutate(
    stdDevV = map(data, function(df) {
      df %>% summarise_if(is.numeric, sd)
    }),
    # meanV = map(data, function(df) {
    #   df %>% summarise_if(is.numeric, mean)
    # }),
    cvV = map(data, function(df) {
      df %>% summarise_if(is.numeric, function(x) {sd(x)/mean(x)})
    }),
    maxV = map(data, function(df) {
      df %>% summarise_if(is.numeric, max)
    })
  ) %>% unnest(c(stdDevV, cvV, maxV), names_sep = "_")

# ggplot(datPSd, aes(y = meanV_tpstotal, x = n)) + 
#   geom_jitter(aes(color = rangeC, shape = interaction(nodesel, varsel, OB))) + 
#   facet_grid(rows = vars(coef), cols = vars(pb), margins = T) +
#   ggtitle("Means") +
#   theme(legend.position="bottom", legend.title=element_blank())
```


```{r, fig.asp=1}
ggplot(datPSd, aes(y = stdDevV_tpstotal, x = n)) + 
  geom_jitter(aes(color = rangeC, shape = interaction(nodesel, varsel, OB))) + 
  facet_grid(rows = vars(coef), cols = vars(pb), margins = T, scales = "free") +
  ggtitle("Std. dev.") +
  theme(legend.position="bottom", legend.title=element_blank())
```


```{r, fig.asp=1}
ggplot(datPSd, aes(y = cvV_tpstotal, x = n)) + 
  geom_jitter(aes(color = rangeC, shape = interaction(nodesel, varsel, OB))) + 
  facet_grid(rows = vars(coef), cols = vars(pb), margins = T, scales = "free") +
  ggtitle("Coeff of variation (sd/mean)") +
  theme(legend.position="bottom", legend.title=element_blank())
```


```{r, fig.asp=1}
ggplot(datPSd, aes(y = maxV_tpstotal, x = n)) + 
  geom_jitter(aes(color = rangeC, shape = interaction(nodesel, varsel, OB))) + 
  facet_grid(rows = vars(coef), cols = vars(pb), margins = T, scales = "free") +
  ggtitle("Max values") +
  theme(legend.position="bottom", legend.title=element_blank()) 
```

In general we may have high variation within instance groups.





## R2d: In which subprocedures are the cpu time used?

```{r}
res <- dat %>% 
  select(instance, namePrefix, insId, constId, pb, n, coef, rangeC, ratioNDcoef, nodesel, varsel, OB, YN, contains("pcttps")) %>% 
  mutate(pcttpsmisc = 100 - pcttpsLB + pcttpsdomi + pcttpsUB + pcttpsnodesel + pcttpsvarsel + pcttpsOB + pcttpsSLUB + pcttpsdomiLUB) %>% 
  group_by(pb) %>% 
  nest()

res <- res %>% 
  mutate(
    meanV = map(data, function(df) {
      df %>% summarise_if(is.numeric, mean)
    }),
  ) %>% unnest(c(meanV), names_sep = "_")
res <- res %>% pivot_longer(contains("pcttps"))
ggplot(res, aes(y = value, x = name, fill = name)) +
  # stat_summary(fun=mean, geom="line", aes(color = win)) +
  geom_col() +
  facet_grid(rows = vars(pb), scales = "fixed") +
  ggtitle("Cpu usage") +
  theme(legend.position="bottom", legend.title=element_blank())

```

